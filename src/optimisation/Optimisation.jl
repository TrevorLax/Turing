module Optimisation

using ..Turing
using ..Turing: ModeResult, MLE, MAP, OptimLogDensity, OptimizationContext, ModeEstimator
using NamedArrays: NamedArray
using Optimization
using OptimizationOptimJL: LBFGS, IPNewton
using DocStringExtensions: TYPEDFIELDS
using Bijectors
using Random
using SciMLBase: OptimizationFunction, OptimizationProblem, AbstractADType, solve

using Accessors: Accessors
using DynamicPPL: Model, DefaultContext, LikelihoodContext

export estimate_mode, maximum_a_posteriori, maximum_likelihood

"""
    ModeEstimationConstraints

A struct that holds constraints for mode estimation problems.

The fields are the same as possible constraints supported by the Optimization.jl:
`ub` and `lb` specify lower and upper bounds of box constraints. `cons` is a function that
takes the parameters of the model and returns a list of derived quantities, which are then
constrained by the lower and upper bounds set by `lcons` and `ucons`. We refer to these
as generic constraints. Please see the documentation of Optimization.jl for more details.

Any of the fields can be `nothing`, disabling the corresponding constraints.
"""
struct ModeEstimationConstraints{
    Lb<:Union{Nothing,AbstractVector},
    Ub<:Union{Nothing,AbstractVector},
    Conns,
    LConns<:Union{Nothing,AbstractVector},
    UConns<:Union{Nothing,AbstractVector},
}
    lb::Lb
    ub::Ub
    cons::Conns
    lcons::LConns
    ucons::UConns
end

has_box_constraints(c::ModeEstimationConstraints) = c.ub !== nothing || c.lb !== nothing
has_generic_constraints(c::ModeEstimationConstraints) = c.cons !== nothing || c.lcons !== nothing || c.ucons !== nothing
has_constraints(c::ModeEstimationConstraints) = has_box_constraints(c) || has_generic_constraints(c)

function ensure_init_value(model::Model, init_value, constraints)
    if init_value !== nothing
        return copy(init_value)
    end
    if has_generic_constraints(constraints)
        throw(ArgumentError("You must provide an initial value when using generic constraints."))
    end
    if has_box_constraints(constraints)
        return [rand(Uniform(lower, upper)) for (lower, upper) in zip(constraints.lb, constraints.ub)]
    end
    return collect(Iterators.flatten(values(rand(model))))
end

"""
    ModeEstimationProblem{M,E,Iv,Cons,OLD,ADType}

A struct that defines a mode estimation problem.

# Fields
$(TYPEDFIELDS)
"""
struct ModeEstimationProblem{
    M<:Model,
    E<:ModeEstimator,
    Iv<:AbstractVector,
    Cons<:ModeEstimationConstraints,
    OLD<:OptimLogDensity,
    ADType<:AbstractADType,
}
    "The DynamicPPL model for which to estimate the mode."
    model::M
    "The mode estimator. Either `MLE()` for maximum likelihood or `MAP()` for maximum a posteriori."
    estimator::E
    "The initial value for the optimization."
    init_value::Iv
    "The constraints for the optimization problem."
    constraints::Cons
    "The `OptimLogDensity` of the model, used as the objective function for the optimization."
    log_density::OLD
    "The automatic differentiation type to use."
    adtype::ADType
    "Whether the objective function \"linked\", i.e. transformed to an unconstrained space."
    linked::Bool
end

"""
    ModeEstimationProblem(model, estimator, init_value, lb, ub, cons, lcons, ucons, adtype)

Create a mode estimation problem.

`lb`, `ub`, `cons`, `lcons`, and `ucons` are constraints for the optimization problem as
described in `ModeEstimationConstraints` and Optimization.jl.

If `init_value` is `nothing`, it is generated by either sampling from the prior distribution
or uniformly from the box constraints.

The problem is always created with `linked=false`. See [`link`](@ref) for how to transform
it.
"""
function ModeEstimationProblem(model, estimator, init_value, lb, ub, cons, lcons, ucons, adtype)
    constraints = ModeEstimationConstraints(lb, ub, cons, lcons, ucons)
    init_value = ensure_init_value(model, init_value, constraints)
    inner_context = estimator isa MAP ? DefaultContext() : LikelihoodContext()
    ctx = OptimizationContext(inner_context)
    log_density = OptimLogDensity(model, ctx)
    return ModeEstimationProblem(model, estimator, init_value, constraints, log_density, adtype, false)
end

has_box_constraints(p::ModeEstimationProblem) = has_box_constraints(p.constraints)
has_generic_constraints(p::ModeEstimationProblem) = has_generic_constraints(p.constraints)
has_constraints(p::ModeEstimationProblem) = has_constraints(p.constraints)

function default_solver(problem::ModeEstimationProblem)
    return has_generic_constraints(problem.constraints) ? IPNewton() : LBFGS()
end

"""
    link(p::ModeEstimationProblem)

Transform a mode estimation problem to unconstrained space, where all parameters can take
any real value.
"""
function link(p::ModeEstimationProblem)
    if has_constraints(p)
        throw(
            "Transforming constrained optimisation problems to unconstrained space is " *
            "not yet implemented."
        )
    end
    # Note that redefining obj and init_value out of place is intentional: It avoids
    # issues with models for which linking changes the parameter space dimension.
    ld = p.log_density
    ld = Accessors.@set ld.varinfo = DynamicPPL.unflatten(ld.varinfo, copy(p.init_value))
    ld = Accessors.@set ld.varinfo = DynamicPPL.link(ld.varinfo, ld.model)
    init_value = ld.varinfo[:]
    return ModeEstimationProblem(
        p.model, p.estimator, init_value, p.constraints, ld, p.adtype, true
    )
end

"""
    OptimizationProblem(me_prob::ModeEstimationProblem)

Create a SciML `OptimizationProblem` from a `ModeEstimationProblem`
"""
function OptimizationProblem(me_prob::ModeEstimationProblem)
    c = me_prob.constraints
    # Note that OptimLogDensity is a callable that evaluates the model with given
    # parameters. Hence we can use it as the objective function.
    f = OptimizationFunction((x, _) -> me_prob.log_density(x), me_prob.adtype; cons=c.cons)
    if !has_constraints(me_prob)
        opt_prob = OptimizationProblem(f, me_prob.init_value)
    else
        opt_prob = OptimizationProblem(
            f, me_prob.init_value;
            lcons=c.lcons, ucons=c.ucons, lb=c.lb, ub=c.ub
        )
    end
    return opt_prob
end

function variable_names(lg::OptimLogDensity)
    return map(Symbol âˆ˜ first, Turing.Inference.getparams(lg.model, lg.varinfo))
end

"""
    ModeResult(prob::ModeEstimationProblem, solution::AbstractVector)

Create a `ModeResult` for a given estimation problem and a solution given by `solve`.

`Optimization.solve` returns its own result type. This function converts that into the
richer format of `ModeResult`. It also takes care of transforming them back to the original
parameter space in case the optimization was done in a transformed space.
"""
function ModeResult(prob::ModeEstimationProblem, solution::AbstractVector)
    solution_values = solution.u
    ld = prob.log_density
    if prob.linked
        ld = Accessors.@set ld.varinfo = DynamicPPL.unflatten(ld.varinfo, solution_values)
        ld = Accessors.@set ld.varinfo = DynamicPPL.invlink(ld.varinfo, ld.model)
        solution_values = ld.varinfo[:]
    end
    # Store the parameters and their names in an array.
    varnames = variable_names(prob.log_density)
    vmat = NamedArray(solution_values, varnames)
    return ModeResult(vmat, solution, -solution.minimum, prob.log_density)
end

"""
    estimate_mode(
        model::Model,
        estimator::ModeEstimator,
        [init_value::Union{AbstractVector,Nothing},
        [solver]];
        adtype::AbstractADType=AutoForwardDiff(),
        <constraints>,
        <extra kwargs>
    )

Find the mode of the probability distribution of a model.

`estimator` can be either `MLE()` for maximum likelihood estimation or `MAP()` for maximum
a posteriori estimation.

`init_value` is the initial value for the optimization. If it is `nothing` or omitted it is
generated by either sampling from the prior distribution or uniformly from the box
constraints.

`solver` is the optimization algorithm to use. It can be any solver recognised by
Optimization.jl. If it is `nothing` or omitted a default solver is chosen.

`<constraints>` refers to the keyword arguments `lb`, `ub`, `cons`, `lcons`, and `ucons`
that define constraints for the optimization problem. Please see the documentation of
`ModeEstimationConstraints` for more details.

Any <extra kwargs> are passed to `Optimization.solve`.
"""
function estimate_mode(
    model::Model,
    estimator::ModeEstimator,
    init_value::Union{AbstractVector,Nothing},
    solver;
    adtype=AutoForwardDiff(),
    cons=nothing,
    lcons=nothing,
    ucons=nothing,
    lb=nothing,
    ub=nothing,
    kwargs...
)
    prob = ModeEstimationProblem(
        model, estimator, init_value, lb, ub, cons, lcons, ucons, adtype
    )
    (solver === nothing) && (solver = default_solver(prob))
    # TODO(mhauru) We currently couple together the questions of whether the user specified
    # bounds/constraints and whether we transform the objective function to an
    # unconstrained space. These should be separate concerns, but for that we need to
    # implement getting the bounds of the prior distributions.
    optimise_in_unconstrained_space = !has_constraints(prob)
    optimise_in_unconstrained_space && (prob = link(prob))
    solution = solve(OptimizationProblem(prob), solver; kwargs...)
    # TODO(mhauru) We return a ModeResult for compatibility with the older Optim.jl
    # interface. Might we want to break that and develop a better return type?
    return ModeResult(prob, solution)
end

function estimate_mode(model::Model, estimator::ModeEstimator, init_value::Union{AbstractVector,Nothing}, args...; kwargs...)
    return estimate_mode(model, estimator, init_value, nothing, args...; kwargs...)
end

function estimate_mode(model::Model, estimator::ModeEstimator, solver, args...; kwargs...)
    return estimate_mode(model, estimator, nothing, solver, args...; kwargs...)
end

function estimate_mode(model::Model, estimator::ModeEstimator, args...; kwargs...)
    return estimate_mode(model, estimator, nothing, nothing, args...; kwargs...)
end

"""
    maximum_a_posteriori(
        model::Model,
        [init_value::Union{AbstractVector,Nothing},
        [solver]];
        adtype::AbstractADType=AutoForwardDiff(),
        <constraints>
        )

Find the maximum a posteriori estimate of a model.

This is a convenience function that calls `estimate_mode` with `MAP()` as the estimator.
Please do see the documentation of `estimate_mode` for more details.
"""
function maximum_a_posteriori(model::Model, args...; kwargs...)
    return estimate_mode(model, MAP(), args...; kwargs...)
end

"""
    maximum_likelihood(
        model::Model,
        [init_value::Union{AbstractVector,Nothing},
        [solver]];
        adtype::AbstractADType=AutoForwardDiff(),
        <constraints>
        )

Find the maximum likelihood estimate of a model.

This is a convenience function that calls `estimate_mode` with `MLE()` as the estimator.
Please do see the documentation of `estimate_mode` for more details.
"""
function maximum_likelihood(model::Model, args...; kwargs...)
    return estimate_mode(model, MLE(), args...; kwargs...)
end

end
