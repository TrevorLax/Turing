{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location": "/archive/", "text": "news archive", "title": "Articles"},{"location": "/feed.xml", "text": "turing jl turing is a universal probabilistic programming language with an intuitive modelling interface composable probabilistic inference and computational scalability http localhost 4000 turing jl tue 23 jul 2019 01 19 52 0700 tue 23 jul 2019 01 19 52 0700 jekyll v3 8 5", "title": ""},{"location": "/", "text": "turing jl turing is a universal probabilistic programming language with an intuitive modelling interface composable probabilistic inference and computational scalability intuitive turing models are easy to read and write specify models quickly and easily universal turing supports models with stochastic control flow models work the way you write them adaptable turing is written fully in julia and can be modified to suit your needs a quick example turing s modelling syntax allows you to specify a model quickly and easily straightforward models can be expressed in the same way as complex hierarchical models with stochastic control flow quick start model gdemo x y begin assumptions inversegamma 2 3 normal 0 sqrt observations x normal sqrt y normal sqrt end large sampling library turing provides hamiltonian monte carlo sampling for differentiable posterior distributions particle mcmc sampling for complex posterior distributions involving discrete variables and stochastic control flow and gibbs sampling which combines particle mcmc hmc and many other mcmc algorithms samplers integrates with other machine learning packages turing supports julia s flux package for automatic differentiation combine turing and flux to construct probabalistic variants of traditional machine learning models bayesian neural network tutorial community join the turing community to contribute learn and get your questions answered github report bugs request features discuss issues and more turing jl discuss browse and join discussions on turing slack discuss advanced topics request access here https slackinvite julialang org ecosystem explore a rich ecosystem of libraries tools and more to support development advancedhmc robust modular and efficient implementation of advanced hamiltonian monte carlo algorithms mcmcchains chain types and utility functions for mcmc simulations bijectors automatic transformations for constrained random variables", "title": "Turing.jl - Turing.jl"},{"location": "/news/", "text": "newssubscribe with rss to keep up with the latest news for site changes see the changelog kept with the code base 9998 update change logwant to see more see the news archive", "title": "News"},{"location": "/search/search_index.json", "text": "config lang en prebuild index false separator s docs for page in site pages unless page excluded in search if added endif assign added false location page url text page content strip html strip newlines slugify ascii replace title page title assign added true endunless endfor for post in site posts unless page excluded in search if added endif assign added false location post url text post content strip html strip newlines slugify ascii replace title post title assign added true endunless endfor for doc in site docs unless doc excluded in search if added endif assign added false location doc url text doc content strip html strip newlines slugify ascii replace title doc title assign added true endunless endfor", "title": ""},{"location": "/sitemap.xml", "text": "now date y m d daily for section in site data toc site baseurl section url now date y m d daily endfor", "title": ""},{"location": "/assets/css/style.css", "text": "import jekyll theme primer", "title": ""},{"location": "/sitemap.xml", "text": "if page xsl endif assign collections site collections where exp collection collection output false for collection in collections assign docs collection docs where exp doc doc sitemap false for doc in docs doc url replace index html absolute url xml escape if doc last modified at or doc date doc last modified at default doc date date to xmlschema endif endfor endfor assign pages site html pages where exp doc doc sitemap false where exp doc doc url 404 html for page in pages page url replace index html absolute url xml escape if page last modified at page last modified at date to xmlschema endif endfor assign static files page static files where exp page page sitemap false where exp page page name 404 html for file in static files file path replace index html absolute url xml escape file modified time date to xmlschema endfor", "title": ""},{"location": "/robots.txt", "text": "sitemap sitemap xml absolute url", "title": ""},{"location": "/feed.xml", "text": "if page xsl endif jekyll site time date to xmlschema page url absolute url xml escape assign title site title default site name if page collection posts assign collection page collection capitalize assign title title append append collection endif if page category assign category page category capitalize assign title title append append category endif if title title smartify xml escape endif if site description site description xml escape endif if site author site author name default site author xml escape if site author email site author email xml escape endif if site author uri site author uri xml escape endif endif assign posts site page collection where exp post post draft true sort date reverse if page category assign posts posts where category page category endif for post in posts limit 10 post title smartify strip html normalize whitespace xml escape post date date to xmlschema post last modified at default post date date to xmlschema post id absolute url xml escape post content strip xml escape assign post author post author default post authors 0 default site author assign post author site data authors post author default post author assign post author email post author email default nil assign post author uri post author uri default nil assign post author name post author name default post author post author name default xml escape if post author email post author email xml escape endif if post author uri post author uri xml escape endif if post category endif for tag in post tags endfor if post excerpt and post excerpt empty post excerpt strip html normalize whitespace xml escape endif assign post image post image path default post image if post image unless post image contains assign post image post image absolute url endunless endif endfor", "title": ""},{"location": "/docs/contributing/guide", "text": "contributingturing is an open source project if you feel that you have some relevant skills and are interested in contributing then please do get in touch you can contribute by opening issues on github or implementing things yourself and making a pull request we would also appreciate example models written using turing turing has a style guide it is not strictly necessary to review it before making a pull request but you may be asked to change portions of your code to conform with the style guide before it is merged how to contributegetting started fork this repository clone your fork on your local machine git clone https github com your username turing jl add a remote corresponding to this repository git remote add upstream https github com turinglang turing jl what can i do look at the issues page to find an outstanding issue for instance you could implement new features fix bugs or write example models git workflowfor more information on how the git workflow typically functions please see the github s introduction or julia s contribution guide", "title": "Contributing"},{"location": "/docs/contributing/style-guide", "text": "style guidethis style guide is adapted from invenia s style guide we would like to thank them for allowing us to access and use it please don t let not having read it stop you from contributing to turing no one will be annoyed if you open a pr whose style doesn t follow these conventions we will just help you correct it before it gets merged these conventions were originally written at invenia taking inspiration from a variety of sources including python s pep8 julia s notes for contributors and julia s style guide what follows is a mixture of a verbatim copy of invenia s original guide and some of our own modifications a word on consistencywhen adhering to this style it s important to realize that these are guidelines and not rules this is stated best in the pep8 a style guide is about consistency consistency with this style guide is important consistency within a project is more important consistency within one module or function is most important but most importantly know when to be inconsistent sometimes the style guide just doesn t apply when in doubt use your best judgment look at other examples and decide what looks best and don t hesitate to ask synopsisattempt to follow both the julia contribution guidelines the julia style guide and this guide when convention guidelines conflict this guide takes precedence known conflicts will be noted in this guide use 4 spaces per indentation level no tabs try to adhere to a 92 character line length limit use upper camel case convention for modules and types use lower case with underscores for method names note julia code likes to use lower case without underscores comments are good try to explain the intentions of the code use whitespace to make the code more readable no whitespace at the end of a line trailing whitespace avoid padding brackets with spaces ex int64 value preferred over int64 value editor configurationsublime text settingsif you are a user of sublime text we recommend that you have the following options in your julia syntax specific settings to modify these settings first open any julia file jl in sublime text then navigate to preferences gt settings more gt syntax specific user translate tabs to spaces true tab size 4 trim trailing white space on save true ensure newline at eof on save true rulers 92 vim settingsif you are a user of vim we recommend that you add the following options to your vimrc file set tabstop 4 sets tabstops to a width of four columns set softtabstop 4 determines the behaviour of tab and backspace keys with expandtab set shiftwidth 4 determines the results of gt gt lt lt and au filetype julia setlocal expandtab replaces tabs with spaces au filetype julia setlocal colorcolumn 93 highlights column 93 to help maintain the 92 character line limit by default vim seems to guess that jl files are written in lisp to ensure that vim recognizes julia files you can manually have it check for the jl extension but a better solution is to install julia vim which also includes proper syntax highlighting and a few cool other features atom settingsatom defaults preferred line length to 80 characters we want that at 92 for julia to change it go to atom gt preferences gt packages search for the language julia package and open the settings for it find preferred line length under julia grammar and change it to 92 code formattingfunction namingnames of functions should describe an action or property irrespective of the type of the argument the argument s type provides this information instead for example buyfood food should be buy food food names of functions should usually be limited to one or two lowercase words ideally write buyfood not buy food but if you are writing a function whose name is hard to read without underscores then please do use them method definitionsonly use short form function definitions when they fit on a single line yes foo x int64 abs x 3 no foobar array data abstractarray t item t where t lt int64 t abs x abs item 3 for x in array data no foobar array data abstractarray t item t where t lt int64 t abs x abs item 3 for x in array data yes function foobar array data abstractarray t item t where t lt int64 return t abs x abs item 3 for x in array data endwhen using long form functions always use the return keyword yes function fnc x t where t result zero t result fna x return resultend no function fnc x t where t result zero t result fna x end yes function foo x y return new x y end no function foo x y new x y endfunctions definitions with parameter lines which exceed 92 characters should separate each parameter by a newline and indent by one level yes function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeend ok function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeend no function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeend no function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeendkeyword argumentswhen calling a function always separate your keyword arguments from your positional arguments with a semicolon this avoids mistakes in ambiguous cases such as splatting a dict yes xy foo x y 3 no xy foo x y 3 whitespaceavoid extraneous whitespace in the following situations immediately inside parentheses square brackets or braces yes spam ham 1 eggs no spam ham 1 eggs immediately before a comma or semicolon yes if x 4 show x y x y y x endno if x 4 show x y x y y x end when using ranges unless additional operators are used yes ham 1 9 ham 1 3 9 ham 1 3 end no ham 1 9 ham 1 3 9 yes ham lower upper ham lower step upper yes ham lower offset upper offset yes ham lower offset upper offset no ham lower offset upper offset more than one space around an assignment or other operator to align it with another yes x 1y 2long variable 3 no x 1y 2long variable 3 always surround these binary operators with a single space on either side assignment updating operators etc numeric comparisons operators lt gt etc note that this guideline does not apply when performing assignment in method definitions yes i i 1no i i 1yes submitted 1no submitted 1yes x 2 lt yno x 2 lt y assignments using expanded array tuple or function notation should have the first open bracket on the same line assignment operator and the closing bracket should match the indentation level of the assignment alternatively you can perform assignments on a single line when they are short yes arr 1 2 3 arr 1 2 3 result function arg1 arg2 arr 1 2 3 no arr 1 2 3 arr 1 2 3 arr 1 2 3 nested array or tuples that are in expanded notation should have the opening and closing brackets at the same indentation level yes x 1 2 3 hello world a b c no y 1 2 3 hello world z 1 2 3 hello world always include the trailing comma when working with expanded arrays tuples or functions notation this allows future edits to easily move elements around or add additional elements the trailing comma should be excluded when the notation is only on a single line yes arr 1 2 3 result function arg1 arg2 arr 1 2 3 no arr 1 2 3 result function arg1 arg2 arr 1 2 3 triple quotes use the indentation of the lowest indented line excluding the opening triple quote this means the closing triple quote should be aligned to least indented line in the string triple backticks should also follow this style even though the indentation does not matter for them yes str hello world str hello world cmd program flag value parameter no str hello world commentscomments should be used to state the intended behaviour of code this is especially important when the code is doing something clever that may not be obvious upon first inspection avoid writing comments that state exactly what the code obviously does yes x x 1 compensate for border no x x 1 increment xcomments that contradict the code are much worse than no comments always make a priority of keeping the comments up to date with code changes comments should be complete sentences if a comment is a phrase or sentence its first word should be capitalized unless it is an identifier that begins with a lower case letter never alter the case of identifiers if a comment is short the period at the end can be omitted block comments generally consist of one or more paragraphs built out of complete sentences and each sentence should end in a period comments should be separated by at least two spaces from the expression and have a single space after the when referencing julia in documentation note that julia refers to the programming language while julia typically in backticks e g julia refers to the executable a commmentcode another commentmore codetododocumentationit is recommended that most modules types and functions should have docstrings that being said only exported functions are required to be documented avoid documenting methods like as the built in docstring for the function already covers the details well try to document a function and not individual methods where possible as typically all methods will have similar docstrings if you are adding a method to a function which was defined in base or another package only add a docstring if the behaviour of your function deviates from the existing docstring docstrings are written in markdown and should be concise docstring lines should be wrapped at 92 characters bar x y compute the bar index between x and y if y is missing compute the bar index betweenall pairs of columns of x function bar x y when types or methods have lots of parameters it may not be feasible to write a concise docstring in these cases it is recommended you use the templates below note if a section doesn t apply or is overly verbose for example throws if your function doesn t throw an exception it can be excluded it is recommended that you have a blank line between the headings and the content when the content is of sufficient length try to be consistent within a docstring whether you use this additional whitespace note that the additional space is only for reading raw markdown and does not effect the rendered version type template should be skipped if is redundant with the constructor s docstring myarray t n my super awesome array wrapper fields data abstractarray t n stores the array being wrapped metadata dict stores metadata about the array struct myarray t n lt abstractarray t n data abstractarray t n metadata dictendfunction template only required for exported functions mysearch array myarray t val t verbose true where t gt intsearches the array for the val for some reason we don t want to use julia sbuiltin search arguments array myarray t the array to search val t the value to search for keywords verbose bool true print out progress details returns int the index where val is located in the array throws notfounderror i guess we could throw an error if val isn t found function mysearch array abstractarray t val t where t endif your method contains lots of arguments or keywords you may want to exclude them from the method signature on the first line and instead use args and or kwargs manager args kwargs gt managera cluster manager which spawns workers arguments min workers integer the minimum number of workers to spawn or an exception is thrown max workers integer the requested number of worker to spawn keywords definition abstractstring name of the job definition to use defaults to the definition used within the current instance name abstractstring queue abstractstring function manager endfeel free to document multiple methods for a function within the same docstring be careful to only do this for functions you have defined manager max workers kwargs manager min workers max workers kwargs manager min workers max workers kwargs a cluster manager which spawns workers arguments min workers int the minimum number of workers to spawn or an exception is thrown max workers int the number of requested workers to spawn keywords definition abstractstring name of the job definition to use defaults to the definition used within the current instance name abstractstring queue abstractstring function manager endif the documentation for bullet point exceeds 92 characters the line should be wrapped and slightly indented avoid aligning the text to the keywords definition abstractstring name of the job definition to use defaults to the definition used within the current instance for additional details on documenting in julia see the official documentation test formattingtestsetsjulia provides test sets which allows developers to group tests into logical groupings test sets can be nested and ideally packages should only have a single root test set it is recommended that the runtests jl file contains the root test set which contains the remainder of the tests testset pkgextreme begin include arithmetic jl include utils jl endthe file structure of the test folder should mirror that of the src folder every file in src should have a complementary file in the test folder containing tests relevant to that file s contents comparisonsmost tests are written in the form test x y since the function doesn t take types into account tests like the following are valid test 1 0 1 avoid adding visual noise into test comparisons yes test value 0 no test value 0 0in cases where you are checking the numerical validity of a model s parameter estimates please use the check numerical function found in test test utils numerical tests jl this function will evaluate a model s parameter estimates using a given tolerance level and test will only be performed if you are running the test suite locally or if travis is executing the numerical testing stage here is an example of usage check that m and s are plus or minus one from 1 5 and 2 2 respectively check numerical chain m s 1 5 2 2 eps 1 0 checks the estimates for a default gdemo model using values 1 5 and 2 0 check gdemo chain eps 0 1 checks the estimates for a default mog model check mogtest default chain eps 0 1", "title": "Style Guide"},{"location": "/docs/library/", "text": "librarymodelling turing core model macro model body macro to specify a probabilistic model example model definition model model generator x default x y begin endexpanded model definition allows passing arguments as kwargsmodel generator x nothing y nothing model generator x y function model generator x nothing y nothing pvars dvars turing get vars tuple x y x x y y data turing get data dvars x x y y defaults turing get default values dvars x default x y nothing inner function sampler turing abstractsampler model inner function model function inner function model return inner function turing varinfo turing samplefromprior model end function inner function vi turing varinfo model return inner function vi turing samplefromprior model end define the main inner function function inner function vi turing varinfo sampler turing abstractsampler model local x if isdefined model data x x model data x else x model defaults x end local y if isdefined model data y y model data y else y model defaults y end vi logp zero real end model turing model pvars dvars inner function data defaults return modelendgenerating a model model generator x value model samplers turing sampler type sampler t generic interface for implementing inference algorithms an implementation of an algorithm should include the following a type specifying the algorithm and its parameters derived from inferencealgorithm a method of sample function that produces results of inference which is where actual inference happens turing translates models to chunks that call the modelling functions at specified points the dispatch is based on the value of a sampler variable to include a new inference algorithm implements the requirements mentioned above in a separate file then include that file at the end of this one turing inference gibbs type gibbs n iters algs compositional mcmc interface gibbs sampling combines one or more sampling algorithms each of which samples from a different set of variables in a model example model gibbs example x begin v1 normal 0 1 v2 categorical 5 end use pg for a v2 variable and use hmc for the v1 variable note that v2 is discrete so the pg sampler is more appropriate than is hmc alg gibbs 1000 hmc 1 0 2 3 v1 pg 20 1 v2 tips hmc and nuts are fast samplers and can throw off particle basedmethods like particle gibbs you can increase the effectiveness of particle sampling by including more particles in the particle sampler turing inference hmc type hmc n iters int float64 n leapfrog int hamiltonian monte carlo sampler arguments n iters int the number of samples to pull float64 the leapfrog step size to use n leapfrog int the number of leapfrop steps to use usage hmc 1000 0 05 10 tips if you are receiving gradient errors when using hmc try reducing thestep size parameter original step sizesample gdemo 1 5 2 hmc 1000 0 1 10 reduced step size sample gdemo 1 5 2 hmc 1000 0 01 10 turing inference hmcda type hmcda n iters int n adapts int float64 float64 init float64 0 1 hamiltonian monte carlo sampler with dual averaging algorithm usage hmcda 1000 200 0 65 0 3 arguments n iters int number of samples to pull n adapts int numbers of samples to use for adaptation float64 target acceptance rate 65 is often recommended float64 target leapfrop length init float64 0 1 inital step size 0 means automatically search by turing for more information please view the following paper arxiv link hoffman matthew d and andrew gelman the no u turn sampler adaptively setting path lengths in hamiltonian monte carlo journal of machine learning research 15 no 1 2014 1593 1623 turing inference ipmcmc type ipmcmc n particles int n iters int n nodes int n csmc nodes int particle gibbs sampler note that this method is particle based and arrays of variables must be stored in a tarray object usage ipmcmc 100 100 4 2 arguments n particles int number of particles to use n iters int number of iterations to employ n nodes int the number of nodes running smc and csmc n csmc nodes int the number of csmc nodes a paper on this can be found here https arxiv org abs 1602 05128 lt a id turing inference is href turing inference is gt lt a gt turing inference is amp mdash type juliais n particles int importance sampling algorithm note that this method is particle based and arrays of variables must be stored in a tarray object arguments n particles is the number of particles to use usage is 1000 example define a simple normal model with unknown mean and variance model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s x 1 normal m sqrt s x 2 normal m sqrt s return s mendsample gdemo 1 5 2 is 1000 turing inference mh type mh n iters int metropolis hastings sampler usage mh 100 m x gt normal x 0 1 example define a simple normal model with unknown mean and variance model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s x 1 normal m sqrt s x 2 normal m sqrt s return s mendchn sample gdemo 1 5 2 mh 1000 turing inference nuts type nuts n iters int n adapts int float64 max depth int 5 max float64 1000 0 init float64 0 1 no u turn sampler nuts sampler usage nuts 1000 200 0 6j max arguments n iters int the number of samples to pull n adapts int the number of samples to use with adapatation float64 target acceptance rate max depth float64 maximum doubling tree depth max float64 maximum divergence during doubling tree init float64 inital step size 0 means automatically search by turing turing inference pg type pg n particles int n iters int particle gibbs sampler note that this method is particle based and arrays of variables must be stored in a tarray object usage pg 100 100 turing inference pmmh type pmmh n iters int smc alg smc parameters algs tuple mh particle independant metropolis hastings and particle marginal metropolis hastings samplers note that this method is particle based and arrays of variables must be stored in a tarray object usage alg pmmh 100 smc 20 v1 mh 1 v2 alg pmmh 100 smc 20 v1 mh 1 v2 x gt normal x 1 arguments n iters int number of iterations to run smc alg smc an smc algorithm to use parameters algs tuple mh an mh algorithm which includes asample space specification turing inference sghmc type sghmc n iters int learning rate float64 momentum decay float64 stochastic gradient hamiltonian monte carlo sampler usage sghmc 1000 0 01 0 1 arguments n iters int number of samples to pull learning rate float64 the learning rate momentum decay float64 momentum decay variable turing inference sgld type sgld n iters int float64 stochastic gradient langevin dynamics sampler usage sgld 1000 0 5 arguments n iters int number of samples to pull float64 the scaling factor for the learing rate reference welling m amp teh y w 2011 bayesian learning via stochastic gradient langevin dynamics in proceedings of the 28th international conference on machine learning icml 11 pp 681 688 turing inference smc type smc n particles int sequential monte carlo sampler note that this method is particle based and arrays of variables must be stored in a tarray object usage smc 1000 data structures libtask tarray type tarray t dims implementation of data structures that automatically perform copy on write after task copying if current task is an existing key in s then return s current task otherwise returns current task s last task usage tarray dim example ta tarray 4 initfor i in 1 4 ta i i end assignarray ta convert to 4 element array int64 1 1 2 3 4 utilities libtask tzeros function tzeros dims construct a distributed array of zeros trailing arguments are the same as those accepted by tarray tzeros dim example tz tzeros 4 constructarray tz convert to 4 element array int64 1 0 0 0 0 index libtask tarray turing inference gibbs turing inference hmc turing inference hmcda turing inference ipmcmc turing inference is turing inference mh turing inference nuts turing inference pg turing inference pmmh turing inference sghmc turing inference sgld turing inference smc turing sampler libtask tzeros turing core model", "title": "Library"},{"location": "/docs/tutorials/0-introduction", "text": "introduction to turingintroductionthis is the first of a series of tutorials on the universal probabilistic programming language turing turing is probabilistic programming system written entirely in julia it has an intuitive modelling syntax and supports a wide range of sampling based inference algorithms most importantly turing inference is composable it combines markov chain sampling operations on subsets of model variables e g using a combination of a hamiltonian monte carlo hmc engine and a particle gibbs pg engine this composable inference engine allows the user to easily switch between black box style inference methods such as hmc and customized inference methods familiarity with julia is assumed through out this tutorial if you are new to julia learning julia is a good starting point for users new to bayesian machine learning please consider more thorough introductions to the field such as pattern recognition and machine learning this tutorial tries to provide an intuition for bayesian inference and gives a simple example on how to use turing note that this is not a comprehensive introduction to bayesian machine learning coin flipping without turingthe following example illustrates the effect of updating our beliefs with every piece of new evidence we observe in particular assume that we are unsure about the probability of heads in a coin flip to get an intuitive understanding of what updating our beliefs is we will visualize the probability of heads in a coin flip after each observed evidence first let s load some of the packages we need to flip a coin random distributions and show our results plots you will note that turing is not an import here we do not need it for this example if you are already familiar with posterior updates you can proceed to the next step using base modules using random load a plotting library using plots load the distributions library using distributionsnext we configure our posterior update model first let s set the true probability that any coin flip will turn up heads and set the number of coin flips we will show our model set the true probability of heads in a coin p true 0 5 iterate from having seen 0 observations to 100 observations ns 0 100 we will now use the bernoulli distribution to flip 100 coins and collect the results in a variable called data draw data from a bernoulli distribution i e draw heads or tails random seed 12 data rand bernoulli p true last ns here s what the first five coin flips look like data 1 5 5 element array int64 1 1 0 1 1 0after flipping all our coins we want to set a prior belief about what we think the distribution of coin flips look like in this case we are going to choose a common prior distribution called the beta distribution our prior belief about the probability of heads in a coin toss prior belief beta 1 1 with our priors set and our data at hand we can perform bayesian inference this is a fairly simple process we expose one additional coin flip to our model every iteration such that the first run only sees the first coin flip while the last iteration sees all the coin flips then we set the updated belief variable to an updated version of the original beta distribution that accounts for the new proportion of heads and tails for the mathematically inclined the beta distribution is updated by adding each coin flip to the distribution s and parameters which are initially defined as over time with more and more coin flips and will be approximately equal to each other as we are equally likely to flip a heads or a tails and the plot of the beta distribution will become more tightly centered around 0 5 this works because mean of the beta distribution is defined as the following text e text beta dfrac alpha alpha beta which is 0 5 when as we expect for a large enough number of coin flips as we increase the number of samples our variance will also decrease such that the distribution will reflect less uncertainty about the probability of receiving a heads the definition of the variance for the beta distribution is the following text var text beta dfrac alpha beta alpha beta 2 alpha beta 1 the intuition about this definition is that the variance of the distribution will approach 0 with more and more samples as the denominator will grow faster than will the numerator more samples means less variance import statsplots for animating purposes using statsplots make an animation animation gif for i n in enumerate ns count the number of heads and tails heads sum data 1 i 1 tails n heads update our prior belief in closed form this is possible because we use a conjugate prior updated belief beta prior belief heads prior belief tails plotting plot updated belief size 500 250 title updated belief after n observations xlabel probability of heads ylabel legend nothing xlim 0 1 fill 0 0 3 w 3 vline p true end the animation above shows that with increasing evidence our belief about the probability of heads in a coin flip slowly adjusts towards the true value the orange line in the animation represents the true probability of seeing heads on a single coin flip while the mode of the distribution shows what the model believes the probability of a heads is given the evidence it has seen coin flipping with turingin the previous example we used the fact that our prior distribution is a conjugate prior note that a closed form expression the updated belief expression for the posterior is not accessible in general and usually does not exist for more interesting models we are now going to move away from the closed form expression above and specify the same model using turing to do so we will first need to import turing mcmcchains distributions and statplots mcmcchains is a library built by the turing team to help summarize markov chain monte carlo mcmc simulations as well as a variety of utility functions for diagnostics and visualizations load turing and mcmcchains using turing mcmcchains load the distributions library using distributions load statsplots for density plots using statsplotsfirst we define the coin flip model using turing model coinflip y begin our prior belief about the probability of heads in a coin p beta 1 1 the number of observations n length y for n in 1 n heads or tails of a coin are drawn from a bernoulli distribution y n bernoulli p endend after defining the model we can approximate the posterior distribution by drawing samples from the distribution in this example we use a hamiltonian monte carlo sampler to draw these samples later tutorials will give more information on the samplers available in turing and discuss their use for different models settings of the hamiltonian monte carlo hmc sampler iterations 1000 0 05 10 start sampling chain sample coinflip data hmc iterations hmc finished with running time 5 194402630000002 accept rate 0 997 lf sample 9 99 evals sample 11 99 pre cond metric 1 0 after finishing the sampling process we can visualize the posterior distribution approximated using turing against the posterior distribution in closed form we can extract the chain data from the sampler using the chains chain p function exported from the mcmcchain module chains chain p creates an instance of the chain type which summarizes the mcmc simulation the mcmcchain module supports numerous tools for plotting summarizing and describing variables of type chain construct summary of the sampling process for the parameter p i e the probability of heads in a coin p summary chain p plot p summary seriestype histogram now we can build our plot compute the posterior distribution in closed form n length data heads sum data updated belief beta prior belief heads prior belief n heads visualize a blue density plot of the approximate posterior distribution using hmc see chain 1 in the legend p plot p summary seriestype density xlim 0 1 legend best w 2 c blue visualize a green density plot of posterior distribution in closed form plot p range 0 stop 1 length 100 pdf ref updated belief range 0 stop 1 length 100 xlabel probability of heads ylabel title xlim 0 1 label closed form fill 0 0 3 w 3 c lightgreen visualize the true probability of heads in red vline p p true label true probability c red as we can see the turing model closely approximates the true probability hopefully this tutorial has provided an easy to follow yet informative introduction to turing s simpler applications more advanced usage will be demonstrated in later tutorials", "title": "Introduction to Turing"},{"location": "/docs/tutorials/1-gaussianmixturemodel", "text": "unsupervised learning using bayesian mixture modelsthe following tutorial illustrates the use turing for clustering data using a bayesian mixture model the aim of this task is to infer a latent grouping hidden structure from unlabelled data more specifically we are interested in discovering the grouping illustrated in figure below this example consists of 2 d data points i e which are distributed according to gaussian distributions for simplicity we use isotropic gaussian distributions but this assumption can easily be relaxed by introducing additional parameters using distributions statsplots random set a random seed random seed 3 construct 30 data points for each cluster n 30 parameters for each cluster we assume that each cluster is gaussian distributed in the example s 3 5 0 0 construct the data points x mapreduce c gt rand mvnormal s c s c 1 n hcat 1 2 visualization scatter x 1 x 2 legend false title synthetic dataset gaussian mixture model in turingto cluster the data points shown above we use a model that consists of two mixture components clusters and assigns each datum to one of the components the assignment thereof determines the distribution that the data point is generated from in particular in a bayesian gaussian mixture model with components for 1 d data each data point with is generated according to the following generative process first we draw the parameters for each cluster i e in our example we draw location of the distributions from a normal mu k sim normal forall k and then draw mixing weight for the clusters from a dirichlet distribution i e w sim dirichlet k alpha after having constructed all the necessary model parameters we can generate an observation by first selecting one of the clusters and then drawing the datum accordingly i e z i sim categorical w forall i x i sim normal mu z i 1 forall i for more details on gaussian mixture models we refer to christopher m bishop pattern recognition and machine learning section 9 using turing mcmcchains turn off the progress monitor turing turnprogress false false model gaussianmixturemodel x begin d n size x draw the paramters for cluster 1 1 normal draw the paramters for cluster 2 2 normal 1 2 uncomment the following lines to draw the weights for the k clusters from a dirichlet distribution 1 0 w dirichlet 2 comment out this line if you instead want to draw the weights w 0 5 0 5 draw assignments for each datum and generate it from a multivariate normal k vector int undef n for i in 1 n k i categorical w x i mvnormal k i k i 1 end return kendgaussianmixturemodel generic function with 2 methods after having specified the model in turing we can construct the model function and run a mcmc simulation to obtain assignments of the data points set the automatic differentiation backend to forward differentiation note this is temporary while the reverse differentiation functionality is being improved turing setadbackend forward diff forward diffgmm model gaussianmixturemodel x to draw observations from the posterior distribution we use a particle gibbs sampler to draw the discrete assignment parameters as well as a hamiltonion monte carlo sampler for continous parameters note that we use a gibbs sampler to combine both samplers for bayesian inference in our model we are also calling mapreduce to generate multiple chains particularly so we test for convergence the chainscat function simply adds multiple chains together gmm sampler gibbs 100 pg 100 1 k hmc 1 0 05 10 1 2 tchain mapreduce c gt sample gmm model gmm sampler chainscat 1 3 visualize the density region of the mixture modelafter sucessfully doing posterior inference we can first visualize the trace and density of the parameters of interest in particular in this example we consider the sample values of the location parameter for the two clusters ids findall map name gt occursin name names tchain p plot tchain ids legend true labels mu 1 mu 2 colordim parameter you ll note here that it appears the location means are switching between chains we will address this in future tutorials for those who are keenly interested see this article on potential solutions for the moment we will just use the first chain to ensure the validity of our inference tchain tchain 1 as the samples for the location parameter for both clusters are unimodal we can safely visualize the density region of our model using the average location helper function used for visualizing the density region function predict x y w use log sum exp trick for numeric stability return turing logaddexp log w 1 logpdf mvnormal 1 1 1 x y log w 2 logpdf mvnormal 2 2 1 x y endpredict generic function with 1 method contour range 5 stop 3 range 6 stop 2 x y gt predict x y 0 5 0 5 mean tchain 1 value mean tchain 2 value scatter x 1 x 2 legend false title synthetic dataset infered assignmentsfinally we can inspect the assignments of the data points infered using turing as we can see the dataset is partitioned into two distinct groups assignments collect skipmissing mean tchain k value dims 1 data scatter x 1 x 2 legend false title assignments on synthetic dataset zcolor assignments", "title": "Unsupervised Learning using Bayesian Mixture Models"},{"location": "/docs/tutorials/2-logisticregression", "text": "bayesian logistic regressionbayesian logistic regression is the bayesian counterpart to a common tool in machine learning logistic regression the goal of logistic regression is to predict a one or a zero for a given training item an example might be predicting whether someone is sick or ill given their symptoms and personal information in our example we ll be working to predict whether someone is likely to default with a synthetic dataset found in the rdatasets package this dataset defaults comes from r s islr package and contains information on borrowers to start let s import all the libraries we ll need import turing and distributions using turing distributions import rdatasets using rdatasets import mcmcchains plots and statsplots for visualizations and diagnostics using mcmcchains plots statsplots we need a logistic function which is provided by statsfuns using statsfuns logistic set a seed for reproducibility using randomrandom seed 0 turn off progress monitor turing turnprogress false falsedata cleaning amp set upnow we re going to import our dataset the first six rows of the dataset are shown below so you capn get a good feel for what kind of data we have import the default dataset data rdatasets dataset islr default show the first six rows of the dataset first data 6 6 4 dataframe row default student balance income categorical categorical float64 float64 1 no no 729 526 44361 6 2 no yes 817 18 12106 1 3 no no 1073 55 31767 1 4 no no 529 251 35704 5 5 no no 785 656 38463 5 6 no yes 919 589 7491 56 most machine learning processes require some effort to tidy up the data and this is no different we need to convert the default and student columns which say yes or no into 1s and 0s afterwards we ll get rid of the old words based columns create new rows defaulted to zero data defaultnum 0 0data studentnum 0 0for i in 1 length data default if a row s default or student columns say yes set them to 1 in our new columns data defaultnum i data default i yes 1 0 0 0 data studentnum i data student i yes 1 0 0 0end delete the old columns which say yes and no deletecols data default deletecols data student show the first six rows of our edited dataset first data 6 6 4 dataframe row balance income defaultnum studentnum float64 float64 float64 float64 1 729 526 44361 6 0 0 0 0 2 817 18 12106 1 0 0 1 0 3 1073 55 31767 1 0 0 0 0 4 529 251 35704 5 0 0 0 0 5 785 656 38463 5 0 0 0 0 6 919 589 7491 56 0 0 1 0 after we ve done that tidying it s time to split our dataset into training and testing sets and separate the labels from the data we separate our data into two halves train and test you can use a higher percentage of splitting or a lower one by modifying the at 0 05 argument we have highlighted the use of only a 5 sample to show the power of bayesian inference with small sample sizes function to split samples function split data df at 0 70 r size df index int round r at train df 1 index test df index 1 end return train testend split our dataset 5 95 into training test sets train test split data data 0 05 create our labels these are the values we are trying to predict train label train defaultnum test label test defaultnum remove the columns that are not our predictors train train studentnum balance income test test studentnum balance income our train and test matrices are still in the dataframe format which tends not to play too well with the kind of manipulations we re about to do so we convert them into matrix objects convert the dataframe objects to matrices train matrix train test matrix test this next part is critically important we must rescale our variables so that they are centered around zero by subtracting each column by the mean and dividing it by the standard deviation without this step turing s sampler will have a hard time finding a place to start searching for parameter estimates rescale our matrices train train mean train dims 1 std train dims 1 test test mean test dims 1 std test dims 1 9500 3 array float64 2 1 54877 0 267577 1 28037 1 54877 2 13084 0 976825 0 645608 0 892311 0 62087 0 645608 0 500971 0 311075 0 645608 1 72494 0 826565 0 645608 0 193203 0 438225 1 54877 0 565783 1 53722 0 645608 0 132822 1 19331 0 645608 0 436599 0 672515 1 54877 0 693263 0 797271 0 645608 0 365488 1 59521 0 645608 0 568975 0 897238 0 645608 0 212375 1 73249 1 54877 1 36916 1 39162 0 645608 0 256626 1 45956 0 645608 0 160862 1 03896 0 645608 0 0195917 1 88261 0 645608 1 51275 0 235979 1 54877 1 31033 1 24868model declarationfinally we can define our model logistic regression takes four arguments x is our set of independent variables y is the element we want to predict n is the number of observations we have and is the standard deviation we want to assume for our priors within the model we create four coefficients intercept student balance and income and assign a prior of normally distributed with means of zero and standard deviations of we want to find values of these four coefficients to predict any given y the for block creates a variable v which is the logistic function we then observe the liklihood of calculating v given the actual label y i bayesian logistic regression lr model logistic regression x y n begin intercept normal 0 student normal 0 balance normal 0 income normal 0 for i 1 n v logistic intercept student x i 1 balance x i 2 income x i 3 y i bernoulli v endend samplingnow we can run our sampler this time we ll use hmc to sample from our posterior this is temporary while the reverse differentiation backend is being improved turing setadbackend forward diff retrieve the number of observations n size train sample using hmc chain mapreduce c gt sample logistic regression train train label n 1 hmc 1500 0 05 10 chainscat 1 3 hmc finished with running time 32 75128093599998 accept rate 0 9946666666666667 lf sample 9 993333333333334 evals sample 0 0006666666666666666 pre cond metric 1 0 hmc finished with running time 32 45210917600001 accept rate 0 9926666666666667 lf sample 9 993333333333334 evals sample 0 0006666666666666666 pre cond metric 1 0 hmc finished with running time 32 587576130999956 accept rate 0 9953333333333333 lf sample 9 993333333333334 evals sample 0 0006666666666666666 pre cond metric 1 0 describe chain 2 element array chaindataframe 1 summary statistics omitted printing of 1 columns row parameters mean std naive se mcse symbol float64 float64 float64 float64 1 balance 1 68562 0 315471 0 00470277 0 00735181 2 income 0 0296337 0 377066 0 00562096 0 00827714 3 intercept 4 3785 0 553123 0 00824547 0 0141316 4 lf eps 0 05 2 0819e 17 3 10351e 19 2 09216e 18 5 student 0 271651 0 373947 0 00557447 0 00935029 quantiles omitted printing of 1 columns row parameters 2 5 25 0 50 0 75 0 symbol float64 float64 float64 float64 1 balance 1 13784 1 48794 1 67625 1 87022 2 income 0 760113 0 280494 0 0298922 0 218566 3 intercept 5 21811 4 62621 4 34761 4 09165 4 lf eps 0 05 0 05 0 05 0 05 5 student 0 995905 0 513987 0 272845 0 0321513 since we ran multiple chains we may as well do a spot check to make sure each chain converges around similar points plot chain looks good we can also use the corner function from mcmcchains to show the distributions of the various parameters of our logistic regression the labels to use l student balance income use the corner function requires statsplots and mcmcchain corner chain l fortunately the corner plot appears to demonstrate unimodal distributions for each of our parameters so it should be straightforward to take the means of each parameter s sampled values to estimate our model to make predictions making predictionshow do we test how well the model actually predicts whether someone is likely to default we need to build a prediction function that takes the test object we made earlier and runs it through the average parameter calculated during sampling the prediction function below takes a matrix and a chain object it takes the mean of each parameter s sampled values and re runs the logistic function using those mean values for every element in the test set function prediction x matrix chain threshold pull the means from each parameter s sampled values in the chain intercept mean chain intercept value student mean chain student value balance mean chain balance value income mean chain income value retrieve the number of rows n size x generate a vector to store our predictions v vector float64 undef n calculate the logistic function for each element in the test set for i in 1 n num logistic intercept student x i 1 balance x i 2 income x i 3 if num gt threshold v i 1 else v i 0 end end return vend let s see how we did we run the test matrix through the prediction function and compute the mean squared error mse for our prediction the threshold variable sets the sensitivity of the predictions for example a threshold of 0 10 will predict a defualt value of 1 for any predicted value greater than 1 0 and no default if it is less than 0 10 set the prediction threshold threshold 0 10 make the predictions predictions prediction test chain threshold calculate mse for our test set loss sum predictions test label 2 length test label 0 08242105263157895perhaps more important is to see what percentage of defaults we correctly predicted the code below simply counts defaults and predictions and presents the results defaults sum test label not defaults length test label defaultspredicted defaults sum test label predictions 1 predicted not defaults sum test label predictions 0 println defaults defaults predictions predicted defaults percentage defaults correct predicted defaults defaults defaults 317 0 predictions 247 percentage defaults correct 0 7791798107255521println not defaults not defaults predictions predicted not defaults percentage non defaults correct predicted not defaults not defaults not defaults 9183 0 predictions 8470 percentage non defaults correct 0 9223565283676358the above shows that with a threshold of 0 10 we correctly predict a respectable portion of the defaults and correctly identify most non defaults this is fairly sensitive to a choice of threshold and you may wish to experiment with it this tutorial has demonstrated how to use turing to perform bayesian logistic regression", "title": "Bayesian Logistic Regression"},{"location": "/docs/tutorials/3-bayesnn", "text": "bayesian neural networksin this tutorial we demonstrate how one can implement a bayesian neural network using a combination of turing and flux a suite of tools machine learning we will use flux to specify the neural network s layers and turing to implement the probabalistic inference with the goal of implementing a classification algorithm we will begin with importing the relevant libraries import libraries using turing flux plots random hide sampling progress turing turnprogress false use reverse diff due to the number of parameters in neural networks turing setadbackend reverse diff reverse diffour goal here is to use a bayesian neural network to classify points in an artificial dataset the code below generates data points arranged in a box like pattern and displays a graph of the dataset we ll be working with number of points to generate n 80m round int n 4 random seed 1234 generate artificial data x1s rand m 4 5 x2s rand m 4 5 xt1s array x1s i 0 5 x2s i 0 5 for i 1 m x1s rand m 4 5 x2s rand m 4 5 append xt1s array x1s i 5 x2s i 5 for i 1 m x1s rand m 4 5 x2s rand m 4 5 xt0s array x1s i 0 5 x2s i 5 for i 1 m x1s rand m 4 5 x2s rand m 4 5 append xt0s array x1s i 5 x2s i 0 5 for i 1 m store all the data for later xs xt1s xt0s ts ones 2 m zeros 2 m plot data points function plot data x1 map e gt e 1 xt1s y1 map e gt e 2 xt1s x2 map e gt e 1 xt0s y2 map e gt e 2 xt0s plots scatter x1 y1 color red clim 0 1 plots scatter x2 y2 color blue clim 0 1 endplot data building a neural networkthe next step is to define a feedforward neural network where we express our parameters as distribtuions and not single points as with traditional neural networks the two functions below unpack and nn forward are helper functions we need when we specify our model in turing unpack takes a vector of parameters and partitions them between weights and biases nn forward constructs a neural network with the variables generated in unpack and returns a prediction based on the weights provided the unpack and nn forward functions are explicity designed to create a neural network with two hidden layers and one output layer as shown below the end of this tutorial provides some code that can be used to generate more general network shapes turn a vector into a set of weights and biases function unpack nn params abstractvector w reshape nn params 1 6 3 2 b reshape nn params 7 9 3 w reshape nn params 10 15 2 3 b reshape nn params 16 17 2 w reshape nn params 18 19 1 2 b reshape nn params 20 20 1 return w b w b w b end construct a neural network using flux and return a predicted value function nn forward xs nn params abstractvector w b w b w b unpack nn params nn chain dense w b tanh dense w b tanh dense w b return nn xs end the probabalistic model specification below creates a params variable which has 20 normally distributed variables each entry in the params vector represents weights and biases of our neural net create a regularization term and a gaussain prior variance term alpha 0 09sig sqrt 1 0 alpha specify the probabalistic model model bayes nn xs ts begin create the weight and bias vector nn params mvnormal zeros 20 sig ones 20 calculate predictions for the inputs given the weights and biases in theta preds nn forward xs nn params observe each prediction for i 1 length ts ts i bernoulli preds i endend inference can now be performed by calling sample we use the hmc sampler here perform inference n 5000ch sample bayes nn hcat xs ts hmc n 0 05 4 hmc finished with running time 244 2390099529995 accept rate 0 9048 lf sample 3 9992 evals sample 5 9992 pre cond metric 1 0 now we extract the weights and biases from the sampled chain we ll use these primarily in determining how good a classifier our model is extract all weight and bias parameters theta ch nn params value data prediction visualizationwe can use map estimation to classify our population by using the set of weights that provided the highest log posterior plot the data we have plot data find the index that provided the highest log posterior in the chain i findmax ch lp value data extract the max row value from i i i i 1 plot the posterior distribution with a contour plot x range collect range 6 stop 6 length 25 y range collect range 6 stop 6 length 25 z nn forward x y theta i 1 for x x range y y range contour x range y range z the contour plot above shows that the map method is not too bad at classifying our data now we can visualize our predictions p tilde x x alpha int theta p tilde x theta p theta x alpha approx sum theta sim p theta x alpha f theta tilde x the nn predict function takes the average predicted value from a network parameterized by weights drawn from the mcmc chain return the average predicted value across multiple weights function nn predict x theta num mean nn forward x theta i 1 for i in 1 10 num end next we use the nn predict function to predict the value at a sample of points where the x and y coordinates range between 6 and 6 as we can see below we still have a satisfactory fit to our data plot the average prediction plot data n end 1500x range collect range 6 stop 6 length 25 y range collect range 6 stop 6 length 25 z nn predict x y theta n end 1 for x x range y y range contour x range y range z if you are interested in how the predictive power of our bayesian neural network evolved between samples the following graph displays an animation of the contour plot generated from the network weights in samples 1 to 1 000 number of iterations to plot n end 500anim animate for i 1 n end plot data z nn forward x y theta i 1 for x x range y y range contour x range y range z title iteration i clim 0 1 end every 5 generic bayesian neural networksthe below code is intended for use in more general applications where you need to be able to change the basic network shape fluidly the code above is highly rigid and adapting it for other architectures would be time consuming currently the code below only supports networks of dense layers here we solve the same problem as above but with three additional 2x2 tanh hidden layers you can modify the network shape variable to specify differing architectures a tuple 3 2 tanh means you want to construct a dense layer with 3 outputs 2 inputs and a tanh activation function you can provide any activation function found in flux by entering it as a symbol e g the tanh function is entered in the third part of the tuple as tanh specify the network architecture network shape 3 2 tanh 2 3 tanh 1 2 regularization parameter variance and total number of parameters alpha 0 09sig sqrt 1 0 alpha num params sum i o i for i o in network shape this modification of the unpack function generates a series of vectors given a network shape function unpack abstractvector network shape abstractvector index 1 weights biases for layer in network shape rows cols layer size rows cols last index w size index 1 last index b last index w rows push weights reshape index last index w rows cols push biases reshape last index w 1 last index b rows index last index b 1 end return weights biasesend generate an abstract neural network given a shape and return a prediction function nn forward x abstractvector network shape abstractvector weights biases unpack network shape layers for i in eachindex network shape push layers dense weights i biases i eval network shape i 3 end nn chain layers return nn x end general turing specification for a bnn model model bayes nn xs ts network shape num params begin mvnormal zeros num params sig ones num params preds nn forward xs network shape for i 1 length ts ts i bernoulli preds i endend set the backend turing setadbackend reverse diff perform inference num samples 500ch2 sample bayes nn hcat xs ts network shape num params nuts num samples 0 65 nuts finished with running time 584 4946359479999 lf sample 0 0 evals sample 179 434 pre cond metric 1 0 1 0 1 0 1 0 1 0 1 0 this function makes predictions based on network shape function nn predict x theta num network shape mean nn forward x theta i network shape 1 for i in 1 10 num end extract the parameters from the sampled chain params2 ch2 value dataplot data x range collect range 6 stop 6 length 25 y range collect range 6 stop 6 length 25 z nn predict x y params2 num samples network shape 1 for x x range y y range contour x range y range z this has been an introduction to the applications of turing and flux in defining bayesian neural networks", "title": "Bayesian Neural Networks"},{"location": "/docs/tutorials/4-bayeshmm", "text": "bayesian hidden markov modelsthis tutorial illustrates training bayesian hidden markov models hmm using turing the main goals are learning the transition matrix emission parameter and hidden states for a more rigorous academic overview on hidden markov models see an introduction to hidden markov models and bayesian networks ghahramani 2001 let s load the libraries we ll need we also set a random seed for reproducibility and the automatic differentiation backend to forward mode more here on why this is useful load libraries using turing plots random turn off progress monitor turing turnprogress false set a random seed and use the forward diff ad mode random seed 1234 turing setadbackend forward diff simple state detectionin this example we ll use something where the states and emission parameters are straightforward define the emission parameter y 1 0 1 0 1 0 1 0 2 0 2 0 2 0 3 0 3 0 3 0 2 0 2 0 2 0 1 0 1 0 n length y k 3 plot the data we just made plot y xlim 0 15 ylim 1 5 size 500 250 we can see that we have three states one for each height of the plot 1 2 3 this height is also our emission parameter so state one produces a value of one state two produces a value of two and so on ultimately we would like to understand three major parameters the transition matrix this is a matrix that assigns a probability of switching from one state to any other state including the state that we are already in the emission matrix which describes a typical value emitted by some state in the plot above the emission parameter for state one is simply one the state sequence is our understanding of what state we were actually in when we observed some data this is very important in more sophisticated hmm models where the emission value does not equal our state with this in mind let s set up our model we are going to use some of our knowledge as modelers to provide additional information about our system this takes the form of the prior on our emission parameter m i sim normal i 0 5 space m 1 2 3 simply put this says that we expect state one to emit values in a normally distributed manner where the mean of each state s emissions is that state s value the variance of 0 5 helps the model converge more quickly consider the case where we have a variance of 1 or 2 in this case the likelihood of observing a 2 when we are in state 1 is actually quite high as it is within a standard deviation of the true emission value applying the prior that we are likely to be tightly centered around the mean prevents our model from being too confused about the state that is generating our observations the priors on our transition matrix are noninformative using t i dirichlet ones k k the dirichlet prior used in this way assumes that the state is likely to change to any other state with equal probability as we ll see this transition matrix prior will be overwritten as we observe data turing model definition model bayeshmm y k begin get observation length n length y state sequence s tzeros int n emission matrix m vector undef k transition matrix t vector vector undef k assign distributions to each element of the transition matrix and the emission matrix for i 1 k t i dirichlet ones k k m i normal i 0 5 end observe each point of the input s 1 categorical k y 1 normal m s 1 0 1 for i 2 n s i categorical vec t s i 1 y i normal m s i 0 1 endend we will use a combination of two samplers hmc and particle gibbs by passing them to the gibbs sampler the gibbs sampler allows for compositional inference where we can utilize different samplers on different parameters in this case we use hmc for m and t representing the emission and transition matrices respectively we use the particle gibbs sampler for s the state sequence you may wonder why it is that we are not assigning s to the hmc sampler and why it is that we need compositional gibbs sampling at all the parameter s is not a continuous variable it is a vector of integers and thus hamiltonian methods like hmc and nuts won t work correctly gibbs allows us to apply the right tools to the best effect if you are a particularly advanced user interested in higher performance you may benefit from setting up your gibbs sampler to use different automatic differentiation backends for each parameter space time to run our sampler g gibbs 1000 hmc 2 0 001 7 m t pg 20 1 s c sample bayeshmm y 3 g let s see how well our chain performed ordinarily using the describe function from mcmcchain would be a good first step but we have generated a lot of parameters here s 1 s 2 m 1 and so on it s a bit easier to show how our model performed graphically the code below generates an animation showing the graph of the data above and the data our model generates in each sample import statsplots for animating purposes using statsplots extract our m and s parameters from the chain m set c m value datas set c s value data iterate through the mcmc samples ns 1 500 make an animation animation animate for i in ns m m set i s int s set i emissions collect skipmissing m s p plot y c red size 500 250 xlabel time ylabel state legend topright label true data xlim 0 15 ylim 1 5 plot emissions color blue label sample n end every 10 looks like our model did a pretty good job but we should also check to make sure our chain converges a quick check is to examine whether the diagonal representing the probability of remaining in the current state of the transition matrix appears to be stationary the code below extracts the diagonal and shows a traceplot of each persistence probability index the chain with the persistence probabilities subchain c t i i for i in 1 k plot the chain plot subchain colordim parameter seriestype traceplot title persistence probability legend right a cursory examination of the traceplot above indicates that at least t 3 3 and possibly t 2 2 have converged to something resembling stationary t 1 1 on the other hand has a slight wobble and seems less consistent than the others we can use the diagnostic functions provided by mcmcchain to engage in some formal tests like the heidelberg and welch diagnostic heideldiag c t heidelberger and welch diagnostic target halfwidth ratio 0 1alpha 0 05parameters burn in stationarity p value mean halfwidth testt 1 1 100 1 0 0969 0 6736 0 0257 1t 1 2 0 1 0 0898 0 2807 0 0216 1t 1 3 300 1 0 5218 0 0455 0 0034 1t 2 1 500 0 0 0002 0 8246 0 0089 1t 2 2 500 0 0 0012 0 1497 0 0098 1t 2 3 200 1 0 1140 0 0249 0 0022 1t 3 1 0 1 0 3270 0 4755 0 0131 1t 3 2 0 1 0 3355 0 4547 0 0120 1t 3 3 500 0 0 0176 0 0612 0 0055 1the p values on the test suggest that we cannot reject the hypothesis that the observed sequence comes from a stationary distribution so we can be somewhat more confident that our transition matrix has converged to something reasonable modifying a model to generate synthetic datawith our learned parameters we can change our model to generate synthetic data you can do this from the first time you specify a model but it is conceptually easier to separate these tasks and avoid muddying the waters in order to create a model that supports this synthetic generating feature there are several changes to your typical model specification that need to be made a general guide can be found here any parameter you were interested in learning before s m t needs to be moved to the argument line of the model assign those parameters default values such as zeros real 10 or whatever is appropriate make sure you add a return line at the end of the model containing the variable s you want to generate in our case this is y and that s about it the code below presents the original bayeshmm model with the necessary changes included generative model model bayeshmm y vector real undef 15 t vector vector real undef 3 m vector real undef 3 k begin get observation length n length y state sequence s tzeros int n observe each point of the input s 1 categorical k y 1 normal m s 1 0 1 for i 2 n s i categorical vec t s i 1 y i normal m s i 0 1 end return yend let s extract the parameters we learned from our chain we re only using the samples starting from 200 to discard the burn in period average t reshape mean c 200 end t value data dims 1 3 3 learned t collect skipmissing average t i for i in 1 3 learned m collect skipmissing mean c 200 end m value data dims 1 finally we can call our model by passing nothing into our parameter of interest and taking a look at it note that we call our model using bayeshmm nothing learned t learned m 3 with an extra set of parentheses at the end for more on this behaviour see this section of the guide focusing on sampling from the prior generate a single sequence generated data bayeshmm nothing learned t learned m 3 plot generated data it doesn t look exactly like our model but it should have all the same properties notice that the spikes to level 3 are quite rare not unlike our original data set", "title": "Bayesian Hidden Markov Models"},{"location": "/docs/tutorials/5-linearregression", "text": "linear regressionturing is powerful when applied to complex hierarchical models but it can also be put to task at common statistical procedures like linear regression this tutorial covers how to implement a linear regression model in turing set upwe begin by importing all the necessary libraries import turing and distributions using turing distributions import rdatasets using rdatasets import mcmcchains plots and statplots for visualizations and diagnostics using mcmcchains plots statsplots set a seed for reproducibility using randomrandom seed 0 hide the progress prompt while sampling turing turnprogress false we will use the mtcars dataset from the rdatasets package mtcars contains a variety of statistics on different car models including their miles per gallon number of cylinders and horsepower among others we want to know if we can construct a bayesian linear regression model to predict the miles per gallon of a car given the other statistics it has lets take a look at the data we have import the default dataset data rdatasets dataset datasets mtcars show the first six rows of the dataset first data 6 6 12 dataframe omitted printing of 6 columns row model mpg cyl disp hp drat string float64 int64 float64 int64 float64 1 mazda rx4 21 0 6 160 0 110 3 9 2 mazda rx4 wag 21 0 6 160 0 110 3 9 3 datsun 710 22 8 4 108 0 93 3 85 4 hornet 4 drive 21 4 6 258 0 110 3 08 5 hornet sportabout 18 7 8 360 0 175 3 15 6 valiant 18 1 6 225 0 105 2 76 size data 32 12 the next step is to get our data ready for testing we ll split the mtcars dataset into two subsets one for training our model and one for evaluating our model then we separate the labels we want to learn mpg in this case and standardize the datasets by subtracting each column s means and dividing by the standard deviation of that column the resulting data is not very familiar looking but this standardization process helps the sampler converge far easier we also create a function called unstandardize which returns the standardized values to their original form we will use this function later on when we make predictions function to split samples function split data df at 0 70 r size df index int round r at train df 1 index test df index 1 end return train testend split our dataset 70 30 into training test sets train test split data data 0 7 save dataframe versions of our dataset train cut dataframe train test cut dataframe test create our labels these are the values we are trying to predict train label train mpg test label test mpg get the list of columns to keep remove names filter x gt in x mpg model names data filter the test and train sets train matrix train remove names test matrix test remove names a handy helper function to rescale our dataset function standardize x return x mean x dims 1 std x dims 1 xend another helper function to unstandardize our datasets function unstandardize x orig return x std orig dims 1 mean orig dims 1 end standardize our dataset train train orig standardize train test test orig standardize test train label train l orig standardize train label test label test l orig standardize test label model specificationin a traditional frequentist model using ols our model might look like mpg i alpha boldsymbol beta t boldsymbol x i where is a vector of coefficients and is a vector of inputs for observation the bayesian model we are more concerned with is the following mpg i sim mathcal n alpha boldsymbol beta t boldsymbol x i sigma 2 where is an intercept term common to all observations is a coefficient vector is the observed data for car and is a common variance term for we assign a prior of truncatednormal 0 100 0 inf this is consistent with andrew gelman s recommendations on noninformative priors for variance the intercept term is assumed to be normally distributed with a mean of zero and a variance of three this represents our assumptions that miles per gallon can be explained mostly by our assorted variables but a high variance term indicates our uncertainty about that each coefficient is assumed to be normally distributed with a mean of zero and a variance of 10 we do not know that our coefficients are different from zero and we don t know which ones are likely to be the most important so the variance term is quite high lastly each observation is distributed according to the calculated mu term given by bayesian linear regression model linear regression x y n obs n vars begin set variance prior truncatednormal 0 100 0 inf set intercept prior intercept normal 0 3 set the priors on our coefficients coefficients array real undef n vars coefficients normal 0 10 calculate all the mu terms mu intercept x coefficients for i 1 n obs y i normal mu i endend with our model specified we can call the sampler we will use the no u turn sampler nuts here n obs n vars size train model linear regression train train label n obs n vars chain sample model nuts 1500 200 0 65 nuts finished with running time 28 76012935999999 lf sample 0 0 evals sample 0 0006666666666666666 pre cond metric 1 0 1 0 1 0 1 0 1 0 1 0 as a visual check to confirm that our coefficients have converged we show the densities and trace plots for our parameters using the plot functionality plot chain it looks like each of our parameters has converged we can check our numerical esimates using describe chain as below describe chain 2 element array chaindataframe 1 summary statistics omitted printing of 1 columns row parameters mean std naive se mcse symbol float64 float64 float64 float64 1 coefficients 1 0 374513 0 44485 0 011486 0 0227265 2 coefficients 2 0 171117 0 476053 0 0122916 0 0225355 3 coefficients 3 0 0681829 0 356122 0 00919503 0 0163717 4 coefficients 4 0 66256 0 33855 0 00874132 0 0141081 5 coefficients 5 0 0969497 0 483806 0 0124918 0 0278113 6 coefficients 6 0 0400533 0 272691 0 00704085 0 016834 7 coefficients 7 0 0995777 0 295442 0 00762827 0 0135998 8 coefficients 8 0 10959 0 313314 0 00808972 0 0171665 9 coefficients 9 0 200219 0 329276 0 00850186 0 0116165 10 coefficients 10 0 682739 0 361389 0 00933104 0 0179951 11 intercept 0 0108571 0 170723 0 00440804 0 0106107 12 lf eps 0 0581085 0 0402024 0 00103802 0 00132349 13 0 484513 0 492571 0 0127181 0 036488 quantiles omitted printing of 1 columns row parameters 2 5 25 0 50 0 75 0 symbol float64 float64 float64 float64 1 coefficients 1 0 497325 0 106879 0 367559 0 650437 2 coefficients 2 1 08863 0 444431 0 174282 0 101657 3 coefficients 3 0 808397 0 294186 0 0607567 0 176866 4 coefficients 4 0 028891 0 453163 0 669321 0 847996 5 coefficients 5 0 848829 0 197623 0 0904946 0 384393 6 coefficients 6 0 495648 0 128853 0 0474724 0 200374 7 coefficients 7 0 662909 0 268329 0 109192 0 0712903 8 coefficients 8 0 421245 0 053784 0 105746 0 24969 9 coefficients 9 0 438313 0 00346737 0 20142 0 408158 10 coefficients 10 1 38271 0 88346 0 679579 0 460576 11 intercept 0 192764 0 0576108 0 00142006 0 0631787 12 lf eps 0 0233708 0 0564162 0 0564162 0 0564162 13 0 293726 0 369497 0 435216 0 508814 comparing to olsa satisfactory test of our model is to evaluate how well it predicts importantly we want to compare our model to existing tools like ols the code below uses the glm jl package to generate a traditional ols multivariate regression on the same data as our probabalistic model import the glm package using glm perform multivariate ols ols lm formula mpg cyl disp hp drat wt qsec vs am gear carb train cut store our predictions in the original dataframe train cut olsprediction glm predict ols test cut olsprediction glm predict ols test cut the function below accepts a chain and an input matrix and calculates predictions we use the mean observation of each parameter in the model starting with sample 200 which is where the warm up period for the nuts sampler ended make a prediction given an input vector function prediction chain x p get params chain 200 end mean p intercept collect mean p coefficients return x endprediction generic function with 2 methods when we make predictions we unstandardize them so they re more understandable we also add them to the original dataframes so they can be placed in context calculate the predictions for the training and testing sets train cut bayespredictions unstandardize prediction chain train train l orig test cut bayespredictions unstandardize prediction chain test test l orig show the first side rows of the modified dataframe first test cut 6 6 14 dataframe omitted printing of 8 columns row model mpg cyl disp hp drat string float64 int64 float64 int64 float64 1 amc javelin 15 2 8 304 0 150 3 15 2 camaro z28 13 3 8 350 0 245 3 73 3 pontiac firebird 19 2 8 400 0 175 3 08 4 fiat x1 9 27 3 4 79 0 66 4 08 5 porsche 914 2 26 0 4 120 3 91 4 43 6 lotus europa 30 4 4 95 1 113 3 77 now let s evaluate the loss for each method and each prediction set we will use sum of squared error function to evaluate loss given by text sse sum y i hat y i 2 where is the actual value true mpg and is the predicted value using either ols or bayesian linear regression a lower sse indicates a closer fit to the data bayes loss1 sum train cut bayespredictions train cut mpg 2 ols loss1 sum train cut olsprediction train cut mpg 2 bayes loss2 sum test cut bayespredictions test cut mpg 2 ols loss2 sum test cut olsprediction test cut mpg 2 println training set bayes loss bayes loss1 ols loss ols loss1test set bayes loss bayes loss2 ols loss ols loss2 training set bayes loss 68 00979321046889 ols loss 67 56037474764624test set bayes loss 242 57948201282844 ols loss 270 94813070761944as we can see above ols and our bayesian model fit our training set about the same this is to be expected given that it is our training set but when we look at our test set we see that the bayesian linear regression model is better able to predict out of sample", "title": "Linear Regression"},{"location": "/docs/tutorials/6-infinitemixturemodel", "text": "probabilistic modelling using the infinite mixture modelin many applications it is desirable to allow the model to adjust its complexity to the amount the data consider for example the task of assigning objects into clusters or groups this task often involves the specification of the number of groups however often times it is not known beforehand how many groups exist moreover in some applictions e g modelling topics in text documents or grouping species the number of examples per group is heavy tailed this makes it impossible to predefine the number of groups and requiring the model to form new groups when data points from previously unseen groups are observed a natural approach for such applications is the use of non parametric models this tutorial will introduce how to use the dirichlet process in a mixture of infinitely many gaussians using turing for further information on bayesian nonparametrics and the dirichlet process we refer to the introduction by zoubin ghahramani and the book fundamentals of nonparametric bayesian inference by subhashis ghosal and aad van der vaart using turingmixture modelbefore introducing infinite mixture models in turing we will briefly review the construction of finite mixture models subsequently we will define how to use the chinese restaurant process construction of a dirichlet process for non parametric clustering two component modelfirst consider the simple case of a mixture model with two gaussian components with fixed covariance the generative process of such a model can be written as where are the mixing weights of the mixture model i e and is a latent assignment of the observation to a component gaussian we can implement this model in turing for 1d data as follows model two model x begin hyper parameters 0 0 0 0 1 0 draw weights 1 beta 1 1 2 1 1 draw locations of the components 1 normal 0 0 2 normal 0 0 draw latent assignment z categorical 1 2 draw observation from selected component if z 1 x normal 1 1 0 else x normal 2 1 0 endendtwo model generic function with 2 methods finite mixture modelif we have more than two components this model can elegantly be extend using a dirichlet distribution as prior for the mixing weights note that the dirichlet distribution is the multivariate generalization of the beta distribution the resulting model can be written as which resembles the model in the gaussian mixture model tutorial with a slightly different notation infinite mixture modelthe question now arises is there a generalization of a dirichlet distribution for which the dimensionality is infinite i e but first to implement an infinite gaussian mixture model in turing we first need to load the turing randommeasures module randommeasures contains a variety of tools useful in nonparametrics using turing randommeasureswe now will utilize the fact that one can integrate out the mixing weights in a gaussian mixture model allowing us to arrive at the chinese restaurant process construction see carl e rasmussen the infinite gaussian mixture model nips 2000 for details in fact if the mixing weights are integrated out the conditional prior for the latent variable is given by p z i k mid z not i alpha frac n k alpha k n 1 alpha where are the latent assignments of all observations except observation note that we use to denote the number of observations at component excluding observation the parameter is the concentration parameter of the dirichlet distribution used as prior over the mixing weights chinese restaurant processto obtain the chinese restaurant process construction we can now derive the conditional prior if for we obtain p z i k mid z not i alpha frac n k n 1 alpha and for all infinitely many clusters that are empty combined we get p z i k mid z not i alpha frac alpha n 1 alpha those equations show that the conditional prior for component assignments is proportional to the number of such observations meaning that the chinese restaurant process has a rich get richer property to get a better understanding of this property we can plot the cluster choosen by for each new observation drawn from the conditional prior concentration parameter 10 0 random measure e g dirichlet process rpm dirichletprocess cluster assignments for each observation z vector int maximum number of observations we observe nmax 500for i in 1 nmax number of observations per cluster k isempty z 0 maximum z nk vector int map k gt sum z k 1 k draw new assignment push z rand chineserestaurantprocess rpm nk endusing plots plot the cluster assignments over time gif for i in 1 nmax scatter collect 1 i z 1 i markersize 2 xlabel observation i ylabel cluster k legend false end further we can see that the number of clusters is logarithmic in the number of observations and data points this is a side effect of the rich get richer phenomenon i e we expect large clusters and thus the number of clusters has to be smaller than the number of observations e k mid n approx alpha log big 1 frac n alpha big we can see from the equation that the concetration parameter allows use to control the number of cluster formed a priori in turing we can implement an infinite gaussian mixture model using the chinese restaurant process construction of a dirichlet process as follows model infinitegmm x begin hyper parameters i e concentration parameter and parameters of h 1 0 0 0 0 0 1 0 define random measure e g dirichlet process rpm dirichletprocess define the base distribution i e expected value of the dirichlet process h normal 0 0 latent assignment z tzeros int length x locations of the infinitely many clusters tzeros float64 0 for i in 1 length x number of clusters k maximum z nk vector int map k gt sum z k 1 k draw the latent assignment z i chineserestaurantprocess rpm nk create a new cluster if z i gt k push 0 0 draw location of new cluster z i h end draw observation x i normal z i 1 0 endendinfinitegmm generic function with 2 methods we can now use turing to infer the assignments of some data points first we will create some random data that comes from three clusters with means of 0 5 and 10 using plots random generate some test data random seed 1 data vcat randn 10 randn 10 5 randn 10 10 data mean data data std data next we ll sample from our posterior using smc mcmc samplingrandom seed 2 iterations 1000model fun infinitegmm data chain sample model fun smc iterations finally we can plot the number of clusters in each sample extract the number of clusters for each sample of the markov chain k map t gt length unique chain z value t 1 iterations visualize the number of clusters plot k xlabel iteration ylabel number of clusters label chain 1 if we visualize the histogram of the number of clusters sampled from our posterior we observe that the model seems to prefer 3 clusters which is the true number of clusters note that the number of clusters in a dirichlet process mixture model is not limited a priori and will grow to infinity with probability one however if conditioned on data the posterior will concentrate on a finite number of clusters enforcing the resulting model to have a finite amount of clusters it is however not given that the posterior of a dirichlet process gaussian mixture model converges to the true number of clusters given that data comes from a finite mixture model see jeffrey miller and matthew harrison a simple example of dirichlet process mixture inconsitency for the number of components for details histogram k xlabel number of clusters legend false one issue with the chinese restaurant process construction is that the number of latent parameters we need to sample scales with the number of observations it may be desirable to use alternative constructions in certain cases alternative methods of constructing a dirichlet process can be employed via the following representations size biased sampling process j k sim beta 1 alpha surplus stick breaking process v k sim beta 1 alpha chinese restaurant process p z n k z 1 n 1 propto begin cases frac m k n 1 alpha text if m k gt 0 frac alpha n 1 alpha end cases for more details see this article", "title": "Probabilistic Modelling using the Infinite Mixture Model"},{"location": "/docs/tutorials/7-poissontegression", "text": "bayesian poisson regressionthis notebook is ported from the example notebook of pymc3 on poisson regression poisson regression is a technique commonly used to model count data some of the applications include predicting the number of people defaulting on their loans or the number of cars running on a highway on a given day this example describes a method to implement the bayesian version of this technique using turing we will generate the dataset that we will be working on which describes the relationship between number of times a person sneezes during the day with his alcohol consumption and medicinal intake we start by importing the required libraries import turing distributions and dataframesusing turing distributions dataframes distributed import mcmcchain plots and statsplots for visualizations and diagnostics using mcmcchains plots statsplots set a seed for reproducibility using randomrandom seed 12 turn off progress monitor turing turnprogress false falsegenerating datawe start off by creating a toy dataset we take the case of a person who takes medicine to prevent excessive sneezing alcohol consumption increases the rate of sneezing for that person thus the two factors affecting the number of sneezes in a given day are alcohol consumption and whether the person has taken his medicine both these variable are taken as boolean valued while the number of sneezes will be a count valued variable we also take into consideration that the interaction between the two boolean variables will affect the number of sneezes5 random rows are printed from the generated data to get a gist of the data generated theta noalcohol meds 1 no alcohol took medicinetheta alcohol meds 3 alcohol took medicinetheta noalcohol nomeds 6 no alcohol no medicinetheta alcohol nomeds 36 alcohol no medicine no of samples for each of the above casesq 100 generate data from different poisson distributionsnoalcohol meds poisson theta noalcohol meds alcohol meds poisson theta alcohol meds noalcohol nomeds poisson theta noalcohol nomeds alcohol nomeds poisson theta alcohol nomeds nsneeze data vcat rand noalcohol meds q rand alcohol meds q rand noalcohol nomeds q rand alcohol nomeds q alcohol data vcat zeros q ones q zeros q ones q meds data vcat zeros q zeros q ones q ones q df dataframe nsneeze nsneeze data alcohol taken alcohol data nomeds taken meds data product alcohol meds meds data alcohol data df sample 1 nrow df 5 replace false 5 4 dataframe row nsneeze alcohol taken nomeds taken product alcohol meds int64 float64 float64 float64 1 8 0 0 1 0 0 0 2 5 1 0 0 0 0 0 3 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 5 38 1 0 1 0 1 0 visualisation of the datasetwe plot the distribution of the number of sneezes for the 4 different cases taken above as expected the person sneezes the most when he has taken alcohol and not taken his medicine he sneezes the least when he doesn t consume alcohol and takes his medicine data plottingp1 plots histogram df df alcohol taken 0 amp df nomeds taken 0 1 title no alcohol meds p2 plots histogram df df alcohol taken 1 amp df nomeds taken 0 1 title alcohol meds p3 plots histogram df df alcohol taken 0 amp df nomeds taken 1 1 title no alcohol no meds p4 plots histogram df df alcohol taken 1 amp df nomeds taken 1 1 title alcohol no meds plot p1 p2 p3 p4 layout 2 2 legend false we must convert our dataframe data into the matrix form as the manipulations that we are about are designed to work with matrix data we also separate the features from the labels which will be later used by the turing sampler to generate samples from the posterior convert the dataframe object to matrices data matrix df alcohol taken nomeds taken product alcohol meds data labels df nsneeze data400 3 array float64 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0we must recenter our data about 0 to help the turing sampler in initialising the parameter estimates so normalising the data in each column by subtracting the mean and dividing by the standard deviation rescale our matrices data data mean data dims 1 std data dims 1 400 3 array float64 2 0 998749 0 998749 0 576628 0 998749 0 998749 0 576628 0 998749 0 998749 0 576628 0 998749 0 998749 0 576628 0 998749 0 998749 0 576628 0 998749 0 998749 0 576628 0 998749 0 998749 0 576628 0 998749 0 998749 0 576628 0 998749 0 998749 0 576628 0 998749 0 998749 0 576628 0 998749 0 998749 1 72988 0 998749 0 998749 1 72988 0 998749 0 998749 1 72988 0 998749 0 998749 1 72988 0 998749 0 998749 1 72988 0 998749 0 998749 1 72988 0 998749 0 998749 1 72988 0 998749 0 998749 1 72988 0 998749 0 998749 1 72988declaring the model poisson regressionour model poisson regression takes four arguments x is our set of independent variables y is the element we want to predict n is the number of observations we have and is the standard deviation we want to assume for our priors within the model we create four coefficients b0 b1 b2 and b3 and assign a prior of normally distributed with means of zero and standard deviations of we want to find values of these four coefficients to predict any given y intuitively we can think of the coefficients as b1 is the coefficient which represents the effect of taking alcohol on the number of sneezes b2 is the coefficient which represents the effect of taking in no medicines on the number of sneezes b3 is the coefficient which represents the effect of interaction between taking alcohol and no medicine on the number of sneezes the for block creates a variable theta which is the weighted combination of the input features we have defined the priors on these weights above we then observe the likelihood of calculating theta given the actual label y i bayesian poisson regression lr model poisson regression x y n begin b0 normal 0 b1 normal 0 b2 normal 0 b3 normal 0 for i 1 n theta b0 b1 x i 1 b2 x i 2 b3 x i 3 y i poisson exp theta endend sampling from the posteriorwe use the nuts sampler to sample values from the posterior we run multiple chains using the mapreduce function to nullify the effect of a problematic chain we then use the gelman rubin and brooks diagnostic to check the convergence of these multiple chains this is temporary while the reverse differentiation backend is being improved turing setadbackend forward diff retrieve the number of observations n size data sample using nuts num chains 4chains mapreduce c gt sample poisson regression data data labels n 10 nuts 2500 200 0 65 chainscat 1 num chains nuts finished with running time 37 545159871000024 lf sample 0 0 evals sample 0 0004 pre cond metric 1 0 1 0 1 0 1 0 nuts finished with running time 365 7509047770001 lf sample 0 0 evals sample 0 0004 pre cond metric 1 0 1 0 1 0 1 0 nuts finished with running time 35 77645984200004 lf sample 0 0 evals sample 0 0004 pre cond metric 1 0 1 0 1 0 1 0 nuts finished with running time 453 8648650210001 lf sample 0 0 evals sample 0 0004 pre cond metric 1 0 1 0 1 0 1 0 viewing the diagnosticswe use the gelman rubin and brooks diagnostic to check whether our chains have converged note that we require multiple chains to use this diagnostic which analyses the difference between these multiple chains we expect the chains to have converged this is because we have taken sufficient number of iterations 1500 for the nuts sampler however in case the test fails then we will have to take a larger number of iterations resulting in longer computation time gelmandiag chains gelman rubin and brooks diagnostic row parameters psrf 97 5 symbol float64 float64 1 b0 1 03468 1 03948 2 b1 1 05339 1 06082 3 b2 1 06552 1 07763 4 b3 1 0341 1 04598 5 lf eps 3 11179 5 30209 from the above diagnostic we can conclude that the chains have converged because the psrf values of the coefficients are close to 1 so we have obtained the posterior distributions of the parameters we transform the coefficients and recover theta values by taking the exponent of the meaned values of the coefficients b0 b1 b2 and b3 we take the exponent of the means to get a better comparison of the relative values of the coefficients we then compare this with the intuitive meaning that was described earlier taking the first chainchain chains 1 calculating the exponentiated meansb0 exp exp mean chain b0 error methoderror no method matching iterate chains union missing float64 missing namedtuple parameters tuple array string 1 namedtuple samples hashedsummary tuple array turing utilities sample 1 base refvalue tuple uint64 chaindataframe closest candidates are iterate matched core simplevector at essentials jl 568 iterate matched core simplevector matched any at essentials jl 568 iterate matched exponentialbackoff at error jl 199 b1 exp exp mean chain b1 error methoderror no method matching iterate chains union missing float64 missing namedtuple parameters tuple array string 1 namedtuple samples hashedsummary tuple array turing utilities sample 1 base refvalue tuple uint64 chaindataframe closest candidates are iterate matched core simplevector at essentials jl 568 iterate matched core simplevector matched any at essentials jl 568 iterate matched exponentialbackoff at error jl 199 b2 exp exp mean chain b2 error methoderror no method matching iterate chains union missing float64 missing namedtuple parameters tuple array string 1 namedtuple samples hashedsummary tuple array turing utilities sample 1 base refvalue tuple uint64 chaindataframe closest candidates are iterate matched core simplevector at essentials jl 568 iterate matched core simplevector matched any at essentials jl 568 iterate matched exponentialbackoff at error jl 199 b3 exp exp mean chain b3 error methoderror no method matching iterate chains union missing float64 missing namedtuple parameters tuple array string 1 namedtuple samples hashedsummary tuple array turing utilities sample 1 base refvalue tuple uint64 chaindataframe closest candidates are iterate matched core simplevector at essentials jl 568 iterate matched core simplevector matched any at essentials jl 568 iterate matched exponentialbackoff at error jl 199 print the exponent of the meaned values of the weights or coefficients are n the exponent of the meaned values of the weights or coefficients are print b0 b0 exp n b1 b1 exp n b2 b2 exp n b3 b3 exp n error undefvarerror b0 exp not definedprint the posterior distributions obtained after sampling can be visualised as n the posterior distributions obtained after sampling can be visualised as visualising the posterior by plotting it plot chains interpreting the obtained mean valuesthe exponentiated mean of the coefficient b1 is roughly half of that of b2 this makes sense because in the data that we generated the number of sneezes was more sensitive to the medicinal intake as compared to the alcohol consumption we also get a weaker dependence on the interaction between the alcohol consumption and the medicinal intake as can be seen from the value of b3 removing the warmup samplesas can be seen from the plots above the parameters converge to their final distributions after a few iterations these initial values during the warmup phase increase the standard deviations of the parameters and are not required after we get the desired distributions thus we remove these warmup values and once again view the diagnostics to remove these warmup values we take all values except the first 200 this is because we set the second parameter of the nuts sampler which is the number of adaptations to be equal to 200 describe chains is used to view the standard deviations in the estimates of the parameters it also gives other useful information such as the means and the quantiles note the standard deviation before removing the warmup samplesdescribe chains 2 element array chaindataframe 1 summary statistics omitted printing of 1 columns row parameters mean std naive se mcse symbol float64 float64 float64 float64 1 b0 1 65616 0 100185 0 00100185 0 00503861 2 b1 0 554498 0 124933 0 00124933 0 00704223 3 b2 0 890379 0 099313 0 00099313 0 00567049 4 b3 0 266706 0 0959565 0 000959565 0 00550219 5 lf eps 0 0117511 0 00881986 8 81986e 5 0 00081771 quantiles omitted printing of 1 columns row parameters 2 5 25 0 50 0 75 0 symbol float64 float64 float64 float64 1 b0 1 60276 1 64499 1 66497 1 68465 2 b1 0 433299 0 513197 0 546945 0 582117 3 b2 0 779827 0 850935 0 883772 0 917298 4 b3 0 168101 0 234231 0 266756 0 300742 5 lf eps 0 00303898 0 0035558 0 00885919 0 0196162 removing the first 200 values of the chainschains new chains 201 2500 describe chains new 2 element array chaindataframe 1 summary statistics omitted printing of 1 columns row parameters mean std naive se mcse symbol float64 float64 float64 float64 1 b0 1 66534 0 0283822 0 000295904 0 00112915 2 b1 0 545759 0 0518339 0 000540406 0 00293611 3 b2 0 882892 0 0491197 0 000512108 0 0027571 4 b3 0 268301 0 0479853 0 000500281 0 00279344 5 lf eps 0 0117602 0 00847608 8 83693e 5 0 000888486 quantiles row parameters 2 5 25 0 50 0 75 0 97 5 symbol float64 float64 float64 float64 float64 1 b0 1 61132 1 64579 1 66522 1 68465 1 72159 2 b1 0 439848 0 513478 0 546904 0 581287 0 642088 3 b2 0 780957 0 851067 0 883823 0 916287 0 974539 4 b3 0 178882 0 234922 0 266776 0 300027 0 363607 5 lf eps 0 00303898 0 0034266 0 011586 0 0199196 0 0208297 visualising the new posterior by plotting it plot chains new as can be seen from the numeric values and the plots above the standard deviation values have decreased and all the plotted values are from the estimated posteriors the exponentiated mean values with the warmup samples removed have not changed by much and they are still in accordance with their intuitive meanings as described earlier", "title": "Bayesian Poisson Regression"},{"location": "/docs/tutorials/index", "text": "tutorialsthis section contains tutorials on how to implement common models in turing if you prefer to have an interactive jupyter notebook please fork or download the turingtutorials repository a list of all the tutorials available can be found to the left the introduction tutorial contains an introduction to coin flipping with turing and a brief overview of probabalistic programming tutorials are under continuous development but there are some older version available at the turingtutorials within the old notebooks section some of these were built using prior versions of turing and may not function correctly but they can assist in the syntax used for common models if there is a tutorial you would like to request please open an issue on the turingtutorials repository", "title": "Tutorials"},{"location": "/docs/using/advanced", "text": "advanced usagehow to define a customized distributionturing jl supports the use of distributions from the distributions jl package by extension it also supports the use of customized distributions by defining them as subtypes of distribution type of the distributions jl package as well as corresponding functions below shows a workflow of how to define a customized distribution using a flat prior as a simple example 1 define the distribution typefirst define a type of the distribution as a subtype of a corresponding distribution type in the distributions jl package struct flat lt continuousunivariatedistribution end2 implement sampling and evaluation of the log pdfsecond define rand and logpdf which will be used to run the model distributions rand d flat rand distributions logpdf t lt real d flat x t zero x 3 define helper functionsin most cases it may be required to define helper functions such as the minimum maximum rand and logpdf functions among others 3 1 domain transformationsome helper functions are necessary for domain transformation for univariate distributions the necessary ones to implement are minimum and maximum distributions minimum d flat infdistributions maximum d flat inffunctions for domain transformation which may be required by multivariate or matrix variate distributions are size d link d x and invlink d x please see turing s transform jl for examples 3 2 vectorization supportthe vectorization syntax follows rv distribution which requires rand and logpdf to be called on multiple data points at once an appropriate implementation for flat are shown below distributions rand d flat n int vector rand for 1 n distributions logpdf t lt real d flat x vector t zero x model internalsthe model macro accepts a function definition and generates a turing model struct for use by the sampler models can be constructed by hand without the use of a macro taking the gdemo model as an example the two code sections below macro and macro free are equivalent using turing model gdemo x begin set priors s inversegamma 2 3 m normal 0 sqrt s observe each value of x x normal m sqrt s endsample gdemo 1 5 2 0 hmc 1000 0 1 5 using turing initialize a namedtuple containing our data variables data x 1 5 2 0 create the model function mf vi sampler model begin set the accumulated logp to zero vi logp 0 if x is provided use the provided values otherwise treat x as an empty vector with two entries if isdefined model data x x model data x else x is a parameter x model defaults x end assume s has an inversegamma distribution s lp turing assume sampler inversegamma 2 3 turing varname c s s vi add the lp to the accumulated logp vi logp lp assume m has a normal distribution m lp turing assume sampler normal 0 sqrt s turing varname c m m vi add the lp to the accumulated logp vi logp lp observe each value of x i according to a normal distribution for i 1 length x vi logp turing observe sampler normal m sqrt s x i vi endend define the default value for x when missingdefaults x vector real undef 2 instantiate a model object model turing model tuple s m tuple x mf data defaults sample the model chain sample model hmc 1000 0 1 5 note that the turing model tuple s m tuple x y accepts two parameter tuples the first set tuple s m represents parameter variables that will be generated by the model while the second tuple x contains the variables to be observed task copyingturing copies julia tasks to deliver efficient inference algorithms but it also provides alternative slower implementation as a fallback task copying is enabled by default task copying requires we use the ctask facility which is provided by libtask to create tasks maximum a posteriori estimationturing does not currently have built in methods for calculating the maximum a posteriori map for a model this is a goal for turing s implementation see this issue but for the moment we present here a method for estimating the map using optim jl using turing define the simple gdemo model model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s return s mendfunction get nlogp model set up the model call sample from the prior vi turing varinfo model define a function to optimize function nlogp sm spl turing samplefromprior new vi turing varinfo vi spl sm model new vi spl new vi logp end return nlogpend define our data points x 1 5y 2 0model gdemo x y nlogp get nlogp model import optim jl using optim create a starting point call the optimizer sm 0 1 0 1 0 lb 0 0 inf ub inf inf result optimize nlogp lb ub sm 0 fminbox parallel samplingturing does not natively support parallel sampling currently users must perform additional structual support note that running chains in parallel may cause unintended issues below is an example of how to run samplers in parallel note that each process must be given a separate seed otherwise the samples generated by independent processes will be equivalent and unhelpful to inference using distributedaddprocs 4 everywhere using turing random set the progress to false to avoid too many notifications everywhere turnprogress false set a different seed for each process for i in procs fetch spawnat i random seed rand int64 end define the model using everywhere everywhere model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endend sampling setup num chains 4sampler nuts 1000 0 65 model gdemo 1 2 3 5 run all samples chns reduce chainscat pmap x gt sample model sampler 1 num chains", "title": "Advanced Usage"},{"location": "/docs/using/autodiff", "text": "automatic differentiationswitching ad modesturing supports two types of automatic differentiation ad in the back end during sampling the current default ad mode is forwarddiff but turing also supports tracker based differentation to switch between forwarddiff and tracker one can call function turing setadbackend backend sym where backend sym can be forward diff or reverse diff compositional sampling with differing ad modesturing supports intermixed automatic differentiation methods for different variable spaces the snippet below shows using forwarddiff to sample the mean m parameter and using the tracker based trackerad autodiff for the variance s parameter using turing define a simple normal model with unknown mean and variance model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end sample using gibbs and varying autodiff backends c sample gdemo 1 5 2 gibbs 1000 hmc turing forwarddiffad 1 2 0 1 5 m hmc turing trackerad 2 0 1 5 s generally trackerad is faster when sampling from variables of high dimensionality greater than 20 and forwarddiffad is more efficient for lower dimension variables this functionality allows those who are performance sensistive to fine tune their automatic differentiation for their specific models if the differentation method is not specified in this way turing will default to using whatever the global ad backend is currently this defaults to forwarddiff", "title": "Automatic Differentiation"},{"location": "/docs/using/dynamichmc", "text": "using dynamichmcturing supports the use of dynamichmc as a sampler through the use of the dynamicnuts function this is a faster version of turing s native nuts implementation dynamicnuts is not appropriate for use in compositional inference if you intend to use gibbs sampling you must use turing s native nuts function to use the dynamicnuts function you must import the dynamichmc package as well as turing turing does not formally require dynamichmc but will include additional functionality if both packages are present here is a brief example of how to apply dynamicnuts import turing and dynamichmc using logdensityproblems dynamichmc turing model definition model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end pull 2 000 samples using dynamicnuts chn sample gdemo 1 5 2 0 dynamicnuts 2000", "title": "Using DynamicHMC"},{"location": "/docs/using/get-started", "text": "getting startedinstallationto use turing you need to install julia first and then install turing install juliayou will need to install julia 1 0 or greater which you can get from the official julia website install turing jlturing is an officially registered julia package so the following will install a stable version of turing while inside julia s package manager press from the repl add turingif you want to use the latest version of turing with some experimental features you can try the following instead add turing mastertest turingif all tests pass you re ready to start using turing examplehere s a simple example showing the package in action using turingusing statsplots define a simple normal model with unknown mean and variance model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end run sampler collect resultschn sample gdemo 1 5 2 hmc 1000 0 1 5 summarise results currently requires the master branch from mcmcchains describe chn plot and save resultsp plot chn savefig gdemo plot png", "title": "Getting Started"},{"location": "/docs/using/guide", "text": "guidebasicsintroductiona probabilistic program is julia code wrapped in a model macro it can use arbitrary julia code but to ensure correctness of inference it should not have external effects or modify global state stack allocated variables are safe but mutable heap allocated objects may lead to subtle bugs when using task copying to help avoid those we provide a turing safe datatype tarray that can be used to create mutable arrays in turing programs to specify distributions of random variables turing programs should use the notation x distr where x is a symbol and distr is a distribution if x is undefined in the model function inside the probabilistic program this puts a random variable named x distributed according to distr in the current scope distr can be a value of any type that implements rand distr which samples a value from the distribution distr if x is defined this is used for conditioning in a style similar to anglican another ppl in this case x is an observed value assumed to have been drawn from the distribution distr the likelihood is computed using logpdf distr y the observe statements should be arranged so that every possible run traverses all of them in exactly the same order this is equivalent to demanding that they are not placed inside stochastic control flow available inference methods include importance sampling is sequential monte carlo smc particle gibbs pg hamiltonian monte carlo hmc hamiltonian monte carlo with dual averaging hmcda and the no u turn sampler nuts simple gaussian demobelow is a simple gaussian demo illustrate the basic usage of turing jl import packages using turingusing statsplots define a simple normal model with unknown mean and variance model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s endnote as a sanity check the expectation of s is 49 24 2 04166666 and the expectation of m is 7 6 1 16666666 we can perform inference by using the sample function the first argument of which is our probabalistic program and the second of which is a sampler more information on each sampler is located in the api run sampler collect results c1 sample gdemo 1 5 2 smc 1000 c2 sample gdemo 1 5 2 pg 10 1000 c3 sample gdemo 1 5 2 hmc 1000 0 1 5 c4 sample gdemo 1 5 2 gibbs 1000 pg 10 2 m hmc 2 0 1 5 s c5 sample gdemo 1 5 2 hmcda 1000 0 15 0 65 c6 sample gdemo 1 5 2 nuts 1000 0 65 the mcmcchains module which is re exported by turing provides plotting tools for the chain objects returned by a sample function see the mcmcchains repository for more information on the suite of tools available for diagnosing mcmc chains summarise resultsdescribe c3 plot resultsplot c3 savefig gdemo plot png the arguments for each sampler are smc number of particles pg number of particles number of iterations hmc number of samples leapfrog step size leapfrog step numbers gibbs number of samples component sampler 1 component sampler 2 hmcda number of samples total leapfrog length target accept ratio nuts number of samples target accept ratio for detailed information on the samplers please review turing jl s api documentation modelling syntax explainedusing this syntax a probabilistic model is defined in turing the model function generated by turing can then be used to condition the model onto data subsequently the sample function can be used to generate samples from the posterior distribution in the following example the defined model is conditioned to the date arg1 1 arg2 2 by passing 1 2 to the model function model model name arg 1 arg 2 begin endthe conditioned model can then be passed onto the sample function to run posterior inference model func model name 1 2 chn sample model func hmc perform inference by sampling using hmc the returned chain contains samples of the variables in the model var 1 mean chn var 1 taking the mean of a variable named var 1 the key var 1 can be a symbol or a string for example to fetch x 1 one can use chn symbol x 1 or chn x 1 the benefit of using a symbol to index allows you to retrieve all the parameters associated with that symbol as an example if you have the parameters x 1 x 2 and x 3 calling chn x will return a new chain with only x 1 x 2 and x 3 turing does not have a declarative form more generally the order in which you place the lines of a model macro matters for example the following example works define a simple normal model with unknown mean and variance model model function y begin s poisson 1 y normal s 1 return yendsample model function 10 smc 100 but if we switch the s poisson 1 and y normal s 1 lines the model will no longer sample correctly define a simple normal model with unknown mean and variance model model function y begin y normal s 1 s poisson 1 return yendsample model function 10 smc 100 sampling multiple chainsif you wish to run multiple chains you can do so with the mapreduce function replace num chains below with however many chains you wish to sample chains mapreduce c gt sample model fun sampler chainscat 1 num chains the chains variable now contains a chains object which can be indexed by chain to pull out the first chain from the chains object use chains 1 having multiple chains in the same object is valuable for evaluating convergence some diagnostic functions like gelmandiag require multiple chains please note that turing does not have native support for chains sampled in parallel sampling from an unconditional distribution the prior turing allows you to sample from a declared model s prior by calling the model without specifying inputs or a sampler in the below example we specify a gdemo model which returns two variables x and y the model includes x and y as arguments but calling the function without passing in x or y means that turing s compiler will assume they are missing values to draw from the relevant distribution the return statement is necessary to retrieve the sampled x and y values model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s return x yendassign the function without inputs to a variable and turing will produce a sample from the prior distribution samples from p x y g prior sampler gdemo g prior sampler output 0 685690547873451 1 1972706455914328 sampling from a conditional distribution the posterior using missingvalues that are missing are treated as parameters to be estimated this can be useful if you want to simulate draws for that parameter or if you are sampling from a conditional distribution turing v0 6 7 supports the following syntax model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endend treat x as a vector of missing values model gdemo fill missing 2 c sample model hmc 500 0 01 5 the above case tells the model compiler the dimensions of the values it needs to generate the generated values for x can be extracted from the chains object using c x currently turing does not support vector valued inputs containing mixed missing and non missing values i e vectors of type union missing t where t is any type the following will not work model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endend warning this will provide an error model gdemo missing 2 4 c sample model hmc 500 0 01 5 if this is functionality you need you may need to define each parameter as a separate variable as below model gdemo x1 x2 begin s inversegamma 2 3 m normal 0 sqrt s note that x1 and x2 are no longer vector valued x1 normal m sqrt s x2 normal m sqrt s end equivalent to sampling p x1 x2 1 5 model gdemo missing 1 5 c sample model hmc 500 0 01 5 using argument defaultsturing models can also be treated as generative by providing default values in the model declaration and then calling that model without arguments suppose we wish to generate data according to the modeleach can be generated by turing in the model below if x is not provided when the function is called x will default to vector real undef 10 a 10 element array of real values the sampler will then treat x as a parameter and generate those quantities using turing declare a model with a default value model generative x vector real undef 10 begin s inversegamma 2 3 m normal 0 sqrt s for i in 1 length x x i normal m sqrt s end return s mendthis model can be called in a traditional fashion with an argument vector of any size the values 1 5 and 2 0 will be observed by the sampler m generative 1 5 2 0 chain sample m hmc 1000 0 01 5 we can generate observations by providing no arguments in the sample call this call will generate a vector of 10 values every sampler iteration generated sample generative hmc 1000 0 01 5 the generated quantities can then be accessed by pulling them out of the chain to access all the x values we first subset the chain using generated x xs generated x you can access the values inside a chain several ways turn them into a dataframe object use their raw axisarray form create a three dimensional array object convert to a dataframe dataframe xs retrieve an axisarray xs value retrieve a basic 3d array xs value datawhat to use as a default valuecurrently the actual value of the default argument does not matter only the dimensions and type of a non atomic value are relevant turing uses default values to pre allocate vectors when they are treated as parameters because if the value is not provided the model will not know the size or type of a vector consider the following model model generator x begin s inversegamma 2 3 m normal 0 sqrt s for i in 1 length x x i normal m sqrt s end return s mendif we are trying to generate random random values from the generator model and we call sample generator hmc 1000 0 01 5 we will receive an error this is because there is no way to determine length x whether x is a vector and the type of the values in x a sensible default value might be model generator x zeros 10 begin s inversegamma 2 3 m normal 0 sqrt s for i in 1 length x x i normal m sqrt s end return s mendin this case the model compiler can now determine that x is a vector float64 1 of length 10 and the model will work as intended it doesn t matter what the values in the vector are at current x will be treated as a parameter if it assumes its default value i e no value was provided in the function call for that variable the element type of the vector or matrix should match the type of the random variable lt integer for discrete random variables and lt abstractfloat for continuous random variables moreover if the continuous random variable is to be sampled using a hamiltonian sampler the vector s element type needs to be real to enable auto differentiation through the model which uses special number types that are sub types of real finally when using a particle sampler a tarray should be used beyond the basicscompositional sampling using gibbsturing jl provides a gibbs interface to combine different samplers for example one can combine an hmc sampler with a pg sampler to run inference for different parameters in a single model as below model simple choice xs begin p beta 2 2 z bernoulli p for i in 1 length xs if z 1 xs i normal 0 1 else xs i normal 2 1 end endendsimple choice f simple choice 1 5 2 0 0 3 chn sample simple choice f gibbs 1000 hmc 1 0 2 3 p pg 20 1 z the gibbs sampler can be used to specify unique automatic differentation backends for different variable spaces please see the automatic differentiation article for more for more details of compositional sampling in turing jl please check the corresponding paper working with mcmcchains jlturing jl wraps its samples using mcmcchains chain so that all the functions working for mcmcchains chain can be re used in turing jl two typical functions are mcmcchains describe and mcmcchains plot which can be used as follows for an obtained chain chn for more information on mcmcchains please see the github repository describe chn lists statistics of the samples plot chn plots statistics of the samples there are numerous functions in addition to describe and plot in the mcmcchains package such as those used in convergence diagnostics for more information on the package please see the github repository working with libtask jlthe libtask jl library provides write on copy data structures that are safe for use in turing s particle based samplers one data structure in particular is often required for use the tarray the following sampler types require the use of a tarray to store distributions ipmcmc is pg pmmh smcif you do not use a tarray to store arrays of distributions when using a particle based sampler you may experience errors here is an example of how the tarray using a tarray constructor function called tzeros can be applied in this way turing model definition model bayeshmm y begin declare a tarray with a length of n s tzeros int n m vector real undef k t vector vector real undef k for i 1 k t i dirichlet ones k k m i normal i 0 01 end draw from a distribution for each element in s s 1 categorical k for i 2 n s i categorical vec t s i 1 y i normal m s i 0 1 end return s m end changing default settingssome of turing jl s default settings can be changed for better usage ad chunk sizeforwarddiff turing s default ad backend uses forward mode chunk wise ad the chunk size can be manually set by setchunksize new chunk size alternatively use an auto tuning helper function auto tune chunk size mf function rep num 10 which will profile various chunk sizes here mf is the model function e g gdemo 1 5 2 and rep num is the number of repetitions during profiling ad backendsince 428 turing jl supports tracker as backend for reverse mode autodiff to switch between forwarddiff jl and tracker one can call function setadbackend backend sym where backend sym can be forward diff or reverse diff for more information on turing s automatic differentiation backend please see the automatic differentiation article progress meterturing jl uses progressmeter jl to show the progress of sampling which may lead to slow down of inference or even cause bugs in some ides due to i o this can be turned on or off by turnprogress true and turnprogress false of which the former is set as default", "title": "Guide"},{"location": "/docs/using/index", "text": "turing documentationwelcome to the documentation for turing 0 6 4 introductionturing is a universal probabilistic programming language with an intuitive modelling interface composable probabilistic inference and computational scalability turing provides hamiltonian monte carlo hmc and particle mcmc sampling algorithms for complex posterior distributions e g those involving discrete variables and stochastic control flows current features include universal probabilistic programming with an intuitive modelling interface hamiltonian monte carlo hmc sampling for differentiable posterior distributions particle mcmc sampling for complex posterior distributions involving discrete variables and stochastic control flow and gibbs sampling that combines particle mcmc hmc and many other mcmc algorithms", "title": "Turing Documentation"},{"location": "/docs/using/quick-start", "text": "probablistic programming in thirty secondsif you are already well versed in probabalistic programming and just want to take a quick look at how turing s syntax works or otherwise just want a model to start with we have provided a bayesian coin flipping model to play with this example can be run on however you have julia installed see getting started but you will need to install the packages turing distributions mcmcchains and statsplots if you have not done so already this is an excerpt from a more formal example introducing probabalistic programming which can be found in jupyter notebook form here or as part of the documentation website here import libraries using turing statsplots random set the true probability of heads in a coin p true 0 5 iterate from having seen 0 observations to 100 observations ns 0 100 draw data from a bernoulli distribution i e draw heads or tails random seed 12 data rand bernoulli p true last ns declare our turing model model coinflip y begin our prior belief about the probability of heads in a coin p beta 1 1 the number of observations n length y for n in 1 n heads or tails of a coin are drawn from a bernoulli distribution y n bernoulli p endend settings of the hamiltonian monte carlo hmc sampler iterations 1000 0 05 10 start sampling chain sample coinflip data hmc iterations construct summary of the sampling process for the parameter p i e the probability of heads in a coin psummary chains chain p histogram psummary", "title": "Probablistic Programming in Thirty Seconds"},{"location": "/docs/using/sampler-viz", "text": "sampler visualizationintroductionthe codefor each sampler we will use the same code to plot sampler paths the block below loads the relevant libraries and defines a function for plotting the sampler s trajectory across the posterior the turing model definition used here is not especially practical but it is designed in such a way as to produce visually interesting posterior surfaces to show how different samplers move along the distribution using plotsusing statsplotsusing turingusing bijectorsusing randomrandom seed 0 define a strange model model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s bumps sin m cos m m m 5 bumps for i in eachindex x x i normal m sqrt s end return s mend define our data points x 1 5 2 0 13 0 2 1 0 0 set up the model call sample from the prior model gdemo x vi turing varinfo model vi turing samplefromprior vi flags trans true false evaluate surface at coordinates function evaluate m1 m2 vi vals m1 m2 model vi turing samplefromprior vi logpendfunction plot sampler chain extract values from chain val get chain s m lp ss link ref inversegamma 2 3 val s ms val m lps val lp how many surface points to sample granularity 500 range start stop points spread 0 5 start minimum ss spread std ss stop maximum ss spread std ss start minimum ms spread std ms stop maximum ms spread std ms rng collect range start stop stop length granularity rng collect range start stop stop length granularity make surface plot p surface rng rng evaluate camera 30 65 ticks nothing colorbar false color inferno line range 1 length ms plot3d ss line range ms line range lps line range lc viridis line z collect line range legend false colorbar false alpha 0 5 return pend samplersgibbsgibbs sampling tends to exhibit a jittery trajectory the example below combines hmc and pg sampling to traverse the posterior c sample model gibbs 1000 hmc 1 0 01 5 s pg 20 1 m plot sampler c other samplers can be combined as well c sample model gibbs 1000 mh 1 s sgld 100 0 01 m plot sampler c hmchamiltonian monte carlo hmc sampling is a typical sampler to use as it tends to be fairly good at converging in a efficient manner it can often be tricky to set the correct parameters for this sampler however and the nuts sampler is often easier to run if you don t want to spend too much time fiddling with step size and and the number of steps to take c sample model hmc 1000 0 01 10 plot sampler c hmcdathe hmcda sampler is an implementation of the hamiltonian monte carlo with dual averaging algorithm found in the paper the no u turn sampler adaptively setting path lengths in hamiltonian monte carlo by hoffman and gelman 2011 the paper can be found on arxiv for the interested reader c sample model hmcda 1000 200 0 65 0 3 plot sampler c mhmetropolis hastings mh sampling is one of the earliest markov chain monte carlo methods mh sampling does not move a lot unlike many of the other samplers implemented in turing typically a much longer chain is required to converge to an appropriate parameter estimate the plot below only uses 1 000 iterations of metropolis hastings c sample model mh 1000 plot sampler c as you can see the mh sampler doesn t move parameter estimates very often nutsthe no u turn sampler nuts is an implementation of the algorithm found in the paper the no u turn sampler adaptively setting path lengths in hamiltonian monte carlo by hoffman and gelman 2011 the paper can be found on arxiv for the interested reader nuts tends to be very good at traversing the minima of complex posteriors quickly c sample model nuts 1000 0 65 plot sampler c the only parameter that needs to be set other than the number of iterations to run is the target acceptance rate in the hoffman and gelman paper they note that a target acceptance rate of 0 65 is typical here is a plot showing a very high acceptance rate note that it appears to stick to a locla minima and is not particularly good at exploring the posterior c sample model nuts 1000 0 95 plot sampler c an exceptionally low acceptance rate will show very few moves on the posterior c sample model nuts 1000 0 2 plot sampler c pgthe particle gibbs pg sampler is an implementation of an algorithm from the paper particle markov chain monte carlo methods by andrieu doucet and holenstein 2010 the interested reader can learn more here the two parameters are the number of particles and the number of iterations the plot below shows the use of 20 particles c sample model pg 20 1000 plot sampler c next we plot using 50 particles c sample model pg 50 1000 plot sampler c pmmhthe particle marginal metropolis hastings pmmh sampler is an implementation of an algorithm from the paper particle markov chain monte carlo methods by andrieu doucet and holenstein 2010 the interested reader can learn more here pmmh supports the use of different samplers across different parameter spaces similar to the gibbs sampler the plot below uses smc and mh c sample model pmmh 1000 smc 20 m mh 10 s plot sampler c pimhin addition to pmmh turing also support the particle independent metropolis hastings pimh pimh accepts a number of iterations and an smc call c sample model pimh 1000 smc 20 plot sampler c sghmcstochastic gradient hamiltonian monte carlo sghmc tends to produce sampling paths not unlike that of stochastic gradient descent in other machine learning model types it is an implementation of an algorithm in the paper stochastic gradient hamiltonian monte carlo by chen fox and guestrin 2014 the interested reader can learn more here this sampler is very similar to the sgld sampler below the two parameters used in sghmc are the learing rate and the momentum decay here is sampler with a higher momentum decay of 0 1 c sample model sghmc 1000 0 001 0 1 plot sampler c and the same sampler with a much lower momentum decay c sample model sghmc 1000 0 001 0 01 plot sampler c sgldthe stochastic gradient langevin dynamics sgld is based on the paper bayesian learning via stochastic gradient langevin dynamics by welling and teh 2011 a link to the article can be found here sgld is an approximation to langevin adjusted mh sgld uses stochastic gradients that are based on mini batches of data and it skips the mh correction step to improve scalability computing metropolis hastings accept probabilities requires evaluation likelihoods for the full dataset making it significantly less scalable the resulting gibbs sampler is no longer unbiased since sgld is an approximate sampler c sample model sgld 1000 0 01 plot sampler c", "title": "Sampler Visualization"}]}
