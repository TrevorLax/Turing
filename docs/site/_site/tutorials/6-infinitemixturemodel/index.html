<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.13.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Probabilistic Modelling using the Infinite Mixture Model - Turing.jl</title>
<meta name="description" content="In many applications it is desirable to allow the model to adjust its complexity to the amount the data. Consider for example the task of assigning objects into clusters or groups. This task often involves the specification of the number of groups. However, often times it is not known beforehand how many groups exist. Moreover, in some applictions, e.g. modelling topics in text documents or grouping species, the number of examples per group is heavy tailed. This makes it impossible to predefine the number of groups and requiring the model to form new groups when data points from previously unseen groups are observed.">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Turing.jl">
<meta property="og:title" content="Probabilistic Modelling using the Infinite Mixture Model">
<meta property="og:url" content="http://localhost:4000/tutorials/6-infinitemixturemodel/">


  <meta property="og:description" content="In many applications it is desirable to allow the model to adjust its complexity to the amount the data. Consider for example the task of assigning objects into clusters or groups. This task often involves the specification of the number of groups. However, often times it is not known beforehand how many groups exist. Moreover, in some applictions, e.g. modelling topics in text documents or grouping species, the number of examples per group is heavy tailed. This makes it impossible to predefine the number of groups and requiring the model to form new groups when data points from previously unseen groups are observed.">







  <meta property="article:published_time" content="2019-06-08T21:56:58-04:00">






<link rel="canonical" href="http://localhost:4000/tutorials/6-infinitemixturemodel/">













<!-- end _includes/seo.html -->



<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">

<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Turing.jl Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="/assets/Documenter.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->


    
      <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Turing.jl </a>
        <!-- Turing.jl -->
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/" >Home</a>
            </li><li class="masthead__menu-item">
              <a href="/docs/" >Documentation</a>
            </li><li class="masthead__menu-item">
              <a href="/tutorials/" >Tutorials</a>
            </li><li class="masthead__menu-item">
              <a href="https://github.com/TuringLang/Turing.jl" >GitHub</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle Menu</label>
  <ul class="nav__items">
    
      <li>
        
          <span class="nav__sub-title">Tutorials</span>
        

        
        <ul>
          
            
            

            
            

            <div class="sidebar-link">
            <li><a href="/tutorials/" class="">Home</a></li>
            </div>

          
            
            

            
            

            <div class="sidebar-link">
            <li><a href="/tutorials/0-introduction/" class="">Introduction to Turing</a></li>
            </div>

          
            
            

            
            

            <div class="sidebar-link">
            <li><a href="/tutorials/1-gaussianmixturemodel/" class="">Gaussian Mixture Models</a></li>
            </div>

          
            
            

            
            

            <div class="sidebar-link">
            <li><a href="/tutorials/2-logisticregression/" class="">Bayesian Logistic Regression</a></li>
            </div>

          
            
            

            
            

            <div class="sidebar-link">
            <li><a href="/tutorials/3-bayesnn/" class="">Bayesian Neural Networks</a></li>
            </div>

          
            
            

            
            

            <div class="sidebar-link">
            <li><a href="/tutorials/4-bayeshmm/" class="">Hidden Markov Models</a></li>
            </div>

          
            
            

            
            

            <div class="sidebar-link">
            <li><a href="/tutorials/5-linearregression/" class="">Linear Regression</a></li>
            </div>

          
            
            

            
            

            <div class="sidebar-link">
            <li><a href="/tutorials/6-infinitemixturemodel/" class="active">Infinite Mixture Models</a></li>
            </div>

          
        </ul>
        
      </li>
    
  </ul>
</nav>

    
  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Probabilistic Modelling using the Infinite Mixture Model">
    <meta itemprop="description" content="In many applications it is desirable to allow the model to adjust its complexity to the amount the data. Consider for example the task of assigning objects into clusters or groups. This task often involves the specification of the number of groups. However, often times it is not known beforehand how many groups exist. Moreover, in some applictions, e.g. modelling topics in text documents or grouping species, the number of examples per group is heavy tailed. This makes it impossible to predefine the number of groups and requiring the model to form new groups when data points from previously unseen groups are observed.">
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Probabilistic Modelling using the Infinite Mixture Model
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> </h4></header>
              <ul class="toc__menu">
  <li><a href="#mixture-model">Mixture Model</a>
     -  <a href="#two-component-model">Two-Component Model</a>
     -  <a href="#finite-mixture-model">Finite Mixture Model</a></li>
  <li><a href="#infinite-mixture-model">Infinite Mixture Model</a>
     -  <a href="#chinese-restaurant-process">Chinese Restaurant Process</a></li>
</ul>
            </nav>
          </aside>
        
        <p>In many applications it is desirable to allow the model to adjust its complexity to the amount the data. Consider for example the task of assigning objects into clusters or groups. This task often involves the specification of the number of groups. However, often times it is not known beforehand how many groups exist. Moreover, in some applictions, e.g. modelling topics in text documents or grouping species, the number of examples per group is heavy tailed. This makes it impossible to predefine the number of groups and requiring the model to form new groups when data points from previously unseen groups are observed.</p>

<p>A natural approach for such applications is the use of non-parametric models. This tutorial will introduce how to use the Dirichlet process in a mixture of infinitely many Gaussians using Turing. For further information on Bayesian nonparametrics and the Dirichlet process we refer to the <a href="http://mlg.eng.cam.ac.uk/pub/pdf/Gha12.pdf">introduction by Zoubin Ghahramani</a> and the book “Fundamentals of Nonparametric Bayesian Inference” by Subhashis Ghosal and Aad van der Vaart.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Turing</span>
</code></pre></div></div>

<h2 id="mixture-model">Mixture Model</h2>

<p>Before introducing infinite mixture models in Turing, we will briefly review the construction of finite mixture models. Subsequently, we will define how to use the <a href="https://en.wikipedia.org/wiki/Chinese_restaurant_process">Chinese restaurant process</a> construction of a Dirichlet process for non-parametric clustering.</p>

<h4 id="two-component-model">Two-Component Model</h4>

<p>First, consider the simple case of a mixture model with two Gaussian components with fixed covariance. 
The generative process of such a model can be written as:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\pi_1 &\sim Beta(a, b) \\
\pi_2 &= 1-\pi_1 \\
\mu_1 &\sim Normal(\mu_0, \Sigma_0) \\
\mu_2 &\sim Normal(\mu_0, \Sigma_0) \\
z_i &\sim Categorical(\pi_1, \pi_2) \\
x_i &\sim Normal(\mu_{z_i}, \Sigma)
\end{align} %]]></script>

<p>where <script type="math/tex">\pi_1, \pi_2</script> are the mixing weights of the mixture model, i.e. <script type="math/tex">\pi_1 + \pi_2 = 1</script>, and <script type="math/tex">z_i</script> is a latent assignment of the observation <script type="math/tex">x_i</script> to a component (Gaussian).</p>

<p>We can implement this model in Turing for 1D data as follows:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@model</span> <span class="n">two_model</span><span class="x">(</span><span class="n">x</span><span class="x">)</span> <span class="o">=</span> <span class="k">begin</span>
    
    <span class="c"># Hyper-parameters</span>
    <span class="n">μ0</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">σ0</span> <span class="o">=</span> <span class="mf">1.0</span>
    
    <span class="c"># Draw weights.</span>
    <span class="n">π1</span> <span class="o">~</span><span class="err"> </span><span class="n">Beta</span><span class="x">(</span><span class="mi">1</span><span class="x">,</span><span class="mi">1</span><span class="x">)</span>
    <span class="n">π2</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">π1</span>
    
    <span class="c"># Draw locations of the components.</span>
    <span class="n">μ1</span> <span class="o">~</span><span class="err"> </span><span class="n">Normal</span><span class="x">(</span><span class="n">μ0</span><span class="x">,</span> <span class="n">σ0</span><span class="x">)</span>
    <span class="n">μ2</span> <span class="o">~</span><span class="err"> </span><span class="n">Normal</span><span class="x">(</span><span class="n">μ0</span><span class="x">,</span> <span class="n">σ0</span><span class="x">)</span>
    
    <span class="c"># Draw latent assignment.</span>
    <span class="n">z</span> <span class="o">~</span><span class="err"> </span><span class="n">Categorical</span><span class="x">([</span><span class="n">π1</span><span class="x">,</span> <span class="n">π2</span><span class="x">])</span>
    
    <span class="c"># Draw observation from selected component.</span>
    <span class="k">if</span> <span class="n">z</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="n">x</span> <span class="o">~</span><span class="err"> </span><span class="n">Normal</span><span class="x">(</span><span class="n">μ1</span><span class="x">,</span> <span class="mf">1.0</span><span class="x">)</span>
    <span class="k">else</span>
        <span class="n">x</span> <span class="o">~</span><span class="err"> </span><span class="n">Normal</span><span class="x">(</span><span class="n">μ2</span><span class="x">,</span> <span class="mf">1.0</span><span class="x">)</span>
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>two_model (generic function with 2 methods)
</code></pre></div></div>

<h4 id="finite-mixture-model">Finite Mixture Model</h4>

<p>If we have more than two components, this model can elegantly be extend using a Dirichlet distribution as prior for the mixing weights <script type="math/tex">\pi_1, \dots, \pi_K</script>. Note that the Dirichlet distribution is the multivariate generalization of the beta distribution. The resulting model can be written as:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
(\pi_1, \dots, \pi_K) &\sim Dirichlet(K, \alpha) \\
\mu_k &\sim Normal(\mu_0, \Sigma_0), \;\; \forall k \\
z &\sim Categorical(\pi_1, \dots, \pi_K) \\
x &\sim Normal(\mu_z, \Sigma) 
\end{align} %]]></script>

<p>which resembles the model in the <a href="1_GaussianMixtureModel.ipynb">Gaussian mixture model tutorial</a> with a slightly different notation.</p>

<h2 id="infinite-mixture-model">Infinite Mixture Model</h2>

<p>The question now arises, is there a generalization of a Dirichlet distribution for which the dimensionality <script type="math/tex">K</script> is infinite, i.e. <script type="math/tex">K = \infty</script>?</p>

<p>But first, to implement an infinite Gaussian mixture model in Turing, we first need to load the <code class="highlighter-rouge">Turing.RandomMeasures</code> module. <code class="highlighter-rouge">RandomMeasures</code> contains a variety of tools useful in nonparametrics.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Turing</span><span class="o">.</span><span class="n">RandomMeasures</span>
</code></pre></div></div>

<p>We now will utilize the fact that one can integrate out the mixing weights in a Gaussian mixture model allowing us to arrive at the Chinese restaurant process construction. See Carl E. Rasmussen: <a href="https://www.seas.harvard.edu/courses/cs281/papers/rasmussen-1999a.pdf">The Infinite Gaussian Mixture Model</a>, NIPS (2000) for details.</p>

<p>In fact, if the mixing weights are integrated out, the conditional prior for the latent variable <script type="math/tex">z</script> is given by:</p>

<p>$$ 
p(z_i = k \mid z_{\not i}, \alpha) = \frac{n_k + \alpha/K}{N - 1 + \alpha}
$$</p>

<p>where <script type="math/tex">z_{\not i}</script> are the latent assignments of all observations except observation <script type="math/tex">i</script>. Note that we use <script type="math/tex">n_k</script> to denote the number of observations at component <script type="math/tex">k</script> excluding observation <script type="math/tex">i</script>. The parameter <script type="math/tex">\alpha</script> is the concentration parameter of the Dirichlet distribution used as prior over the mixing weights.</p>

<h4 id="chinese-restaurant-process">Chinese Restaurant Process</h4>

<p>To obtain the Chinese restaurant process construction, we can now derive the conditional prior if <script type="math/tex">K \rightarrow \infty</script>.</p>

<p>For <script type="math/tex">n_k > 0</script> we obtain:</p>

<p>$$
p(z_i = k \mid z_{\not i}, \alpha) = \frac{n_k}{N - 1 + \alpha}
$$</p>

<p>and for all infinitely many clusters that are empty (combined) we get:</p>

<p>$$
p(z_i = k \mid z_{\not i}, \alpha) = \frac{\alpha}{N - 1 + \alpha}
$$</p>

<p>Those equations show that the conditional prior for component assignments is proportional to the number of such observations, meaning that the Chinese restaurant process has a rich get richer property.</p>

<p>To get a better understanding of this property, we can plot the cluster choosen by for each new observation drawn from the conditional prior.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Concentration parameter.</span>
<span class="n">α</span> <span class="o">=</span> <span class="mf">10.0</span>

<span class="c"># Random measure, e.g. Dirichlet process.</span>
<span class="n">rpm</span> <span class="o">=</span> <span class="n">DirichletProcess</span><span class="x">(</span><span class="n">α</span><span class="x">)</span>

<span class="c"># Cluster assignments for each observation.</span>
<span class="n">z</span> <span class="o">=</span> <span class="kt">Vector</span><span class="x">{</span><span class="kt">Int</span><span class="x">}()</span>

<span class="c"># Maximum number of observations we observe.</span>
<span class="n">Nmax</span> <span class="o">=</span> <span class="mi">500</span>

<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">Nmax</span>
    <span class="c"># Number of observations per cluster.</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">isempty</span><span class="x">(</span><span class="n">z</span><span class="x">)</span> <span class="o">?</span> <span class="mi">0</span> <span class="o">:</span> <span class="n">maximum</span><span class="x">(</span><span class="n">z</span><span class="x">)</span>
    <span class="n">nk</span> <span class="o">=</span> <span class="kt">Vector</span><span class="x">{</span><span class="kt">Int</span><span class="x">}(</span><span class="n">map</span><span class="x">(</span><span class="n">k</span> <span class="o">-&gt;</span> <span class="n">sum</span><span class="x">(</span><span class="n">z</span> <span class="o">.==</span> <span class="n">k</span><span class="x">),</span> <span class="mi">1</span><span class="o">:</span><span class="n">K</span><span class="x">))</span>
    
    <span class="c"># Draw new assignment.</span>
    <span class="n">push!</span><span class="x">(</span><span class="n">z</span><span class="x">,</span> <span class="n">rand</span><span class="x">(</span><span class="n">ChineseRestaurantProcess</span><span class="x">(</span><span class="n">rpm</span><span class="x">,</span> <span class="n">nk</span><span class="x">)))</span>
<span class="k">end</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Plots</span>

<span class="c"># Plot the cluster assignments over time </span>
<span class="nd">@gif</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">Nmax</span>
    <span class="n">scatter</span><span class="x">(</span><span class="n">collect</span><span class="x">(</span><span class="mi">1</span><span class="o">:</span><span class="n">i</span><span class="x">),</span> <span class="n">z</span><span class="x">[</span><span class="mi">1</span><span class="o">:</span><span class="n">i</span><span class="x">],</span> <span class="n">markersize</span> <span class="o">=</span> <span class="mi">2</span><span class="x">,</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="s">"observation (i)"</span><span class="x">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s">"cluster (k)"</span><span class="x">,</span> <span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="x">)</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/422990/55284032-90cfa980-5323-11e9-8a99-f9315db170cb.gif" alt="tmp" /></p>

<p>Further, we can see that the number of clusters is logarithmic in the number of observations and data points. This is a side-effect of the “rich get richer” phenomenon, i.e. we expect large clusters and thus the number of clusters has to be smaller than the number of observations.</p>

<p>$$ 
E[K \mid N] \approx \alpha * log \big(1 - \frac{N}{\alpha}\big)
$$</p>

<p>We can see from the equation that the concetration parameter <script type="math/tex">\alpha</script> allows use to control the number of cluster formed a priori.</p>

<p>In Turing we can implement an infinite Gaussian mixture model using the Chinese restaurant process construction of a Dirichlet process as follows:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@model</span> <span class="n">infiniteGMM</span><span class="x">(</span><span class="n">x</span><span class="x">)</span> <span class="o">=</span> <span class="k">begin</span>
    
    <span class="c"># Hyper-parameters, i.e. concentration parameter and parameters of H.</span>
    <span class="n">α</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">μ0</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">σ0</span> <span class="o">=</span> <span class="mf">1.0</span>
    
    <span class="c"># Define random measure, e.g. Dirichlet process.</span>
    <span class="n">rpm</span> <span class="o">=</span> <span class="n">DirichletProcess</span><span class="x">(</span><span class="n">α</span><span class="x">)</span>
    
    <span class="c"># Define the base distribution, i.e. expected value of the Dirichlet process.</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">Normal</span><span class="x">(</span><span class="n">μ0</span><span class="x">,</span> <span class="n">σ0</span><span class="x">)</span>
    
    <span class="c"># Latent assignment.</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tzeros</span><span class="x">(</span><span class="kt">Int</span><span class="x">,</span> <span class="n">length</span><span class="x">(</span><span class="n">x</span><span class="x">))</span>
        
    <span class="c"># Locations of the infinitely many clusters.</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">tzeros</span><span class="x">(</span><span class="kt">Float64</span><span class="x">,</span> <span class="mi">0</span><span class="x">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
        
        <span class="c"># Number of clusters.</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">maximum</span><span class="x">(</span><span class="n">z</span><span class="x">)</span>
        <span class="n">nk</span> <span class="o">=</span> <span class="kt">Vector</span><span class="x">{</span><span class="kt">Int</span><span class="x">}(</span><span class="n">map</span><span class="x">(</span><span class="n">k</span> <span class="o">-&gt;</span> <span class="n">sum</span><span class="x">(</span><span class="n">z</span> <span class="o">.==</span> <span class="n">k</span><span class="x">),</span> <span class="mi">1</span><span class="o">:</span><span class="n">K</span><span class="x">))</span>

        <span class="c"># Draw the latent assignment.</span>
        <span class="n">z</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span><span class="err"> </span><span class="n">ChineseRestaurantProcess</span><span class="x">(</span><span class="n">rpm</span><span class="x">,</span> <span class="n">nk</span><span class="x">)</span>
        
        <span class="c"># Create a new cluster?</span>
        <span class="k">if</span> <span class="n">z</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">&gt;</span> <span class="n">K</span>
            <span class="n">push!</span><span class="x">(</span><span class="n">μ</span><span class="x">,</span> <span class="mf">0.0</span><span class="x">)</span>

            <span class="c"># Draw location of new cluster.</span>
            <span class="n">μ</span><span class="x">[</span><span class="n">z</span><span class="x">[</span><span class="n">i</span><span class="x">]]</span> <span class="o">~</span><span class="err"> </span><span class="n">H</span>
        <span class="k">end</span>
                
        <span class="c"># Draw observation.</span>
        <span class="n">x</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">μ</span><span class="x">[</span><span class="n">z</span><span class="x">[</span><span class="n">i</span><span class="x">]],</span> <span class="mf">1.0</span><span class="x">)</span>
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>infiniteGMM (generic function with 2 methods)
</code></pre></div></div>

<p>We can now use Turing to infer the assignments of some data points. First, we will create some random data that comes from three clusters, with means of 0, -5, and 10.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Plots</span><span class="x">,</span> <span class="n">Random</span>

<span class="c"># Generate some test data.</span>
<span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="x">(</span><span class="mi">1</span><span class="x">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="n">randn</span><span class="x">(</span><span class="mi">10</span><span class="x">),</span> <span class="n">randn</span><span class="x">(</span><span class="mi">10</span><span class="x">)</span> <span class="o">.-</span> <span class="mi">5</span><span class="x">,</span> <span class="n">randn</span><span class="x">(</span><span class="mi">10</span><span class="x">)</span> <span class="o">.+</span> <span class="mi">10</span><span class="x">)</span>
<span class="n">data</span> <span class="o">.-=</span> <span class="n">mean</span><span class="x">(</span><span class="n">data</span><span class="x">)</span>
<span class="n">data</span> <span class="o">/=</span> <span class="n">std</span><span class="x">(</span><span class="n">data</span><span class="x">);</span>
</code></pre></div></div>

<p>Next, we’ll sample from our posterior using SMC.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># MCMC sampling</span>
<span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="x">(</span><span class="mi">2</span><span class="x">)</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">model_fun</span> <span class="o">=</span> <span class="n">infiniteGMM</span><span class="x">(</span><span class="n">data</span><span class="x">);</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">model_fun</span><span class="x">,</span> <span class="n">SMC</span><span class="x">(</span><span class="n">iterations</span><span class="x">));</span>
</code></pre></div></div>

<p>Finally, we can plot the number of clusters in each sample.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Extract the number of clusters for each sample of the Markov chain.</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">map</span><span class="x">(</span><span class="n">t</span> <span class="o">-&gt;</span> <span class="n">length</span><span class="x">(</span><span class="n">unique</span><span class="x">(</span><span class="n">chain</span><span class="x">[</span><span class="o">:</span><span class="n">z</span><span class="x">]</span><span class="o">.</span><span class="n">value</span><span class="x">[</span><span class="n">t</span><span class="x">,</span><span class="o">:</span><span class="x">,</span><span class="o">:</span><span class="x">])),</span> <span class="mi">1</span><span class="o">:</span><span class="n">iterations</span><span class="x">);</span>

<span class="c"># Visualize the number of clusters.</span>
<span class="n">plot</span><span class="x">(</span><span class="n">k</span><span class="x">,</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="s">"Iteration"</span><span class="x">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s">"Number of clusters"</span><span class="x">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"Chain 1"</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="/tutorials/figures/6_InfiniteMixtureModel_9_1.png" alt="" /></p>

<p>If we visualize the histogram of the number of clusters sampled from our posterior, we observe that the model seems to prefer 3 clusters, which is the true number of clusters. Note that the number of clusters in a Dirichlet process mixture model is not limited a priori and will grow to infinity with probability one. However, if conditioned on data the posterior will concentrate on a finite number of clusters enforcing the resulting model to have a finite amount of clusters. It is, however, not given that the posterior of a Dirichlet process Gaussian mixture model converges to the true number of clusters, given that data comes from a finite mixture model. See Jeffrey Miller and Matthew Harrison: <a href="https://arxiv.org/pdf/1301.2708.pdf">A simple example of Dirichlet process mixture inconsitency for the number of components</a> for details.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">histogram</span><span class="x">(</span><span class="n">k</span><span class="x">,</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="s">"Number of clusters"</span><span class="x">,</span> <span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="/tutorials/figures/6_InfiniteMixtureModel_10_1.png" alt="" /></p>

<p>One issue with the Chinese restaurant process construction is that the number of latent parameters we need to sample scales with the number of observations. It may be desirable to use alternative constructions in certain cases. Alternative methods of constructing a Dirichlet process can be employed via the following representations:</p>

<p>Size-Biased Sampling Process</p>

<p>$$
j_k \sim Beta(1, \alpha) * surplus
$$</p>

<p>Stick-Breaking Process
$$
v_k \sim Beta(1, \alpha)
$$</p>

<p>Chinese Restaurant Process
$$
p(z_n = k | z_{1:n-1}) \propto \begin{cases} 
        \frac{m_k}{n-1+\alpha}, \text{ if } m_k &gt; 0\\
        \frac{\alpha}{n-1+\alpha}
    \end{cases}
$$</p>

<p>For more details see <a href="https://www.stats.ox.ac.uk/~teh/research/npbayes/Teh2010a.pdf">this article</a>.</p>

        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/tutorials/4-bayeshmm/" class="pagination--pager prev" title="Bayesian Hidden Markov Models
">Previous: Bayesian Hidden Markov Models
</a>
    
    
      <a href="/tutorials/" class="pagination--pager next" title="Tutorials
">Next: Tutorials
</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>
      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <!-- 
<div class="page__footer-copyright">&copy; 2019 Turing.jl
 -->
      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>





  </body>
</html>
