I"<N<h1 id="unsupervised-learning-using-bayesian-mixture-models">Unsupervised Learning using Bayesian Mixture Models</h1>

<p>The following tutorial illustrates the use <em>Turing</em> for clustering data using a Bayesian mixture model. The aim of this task is to infer a latent grouping (hidden structure) from unlabelled data.</p>

<p>More specifically, we are interested in discovering the grouping illustrated in figure below. This example consists of 2-D data points, i.e. \(\boldsymbol{x} = \{x_i\}_{i=1}^N \,, x_i \in \mathcal{R}^2\), which are distributed according to Gaussian distributions. For simplicity, we use isotropic Gaussian distributions but this assumption can easily be relaxed by introducing additional parameters.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Distributions</span><span class="x">,</span> <span class="n">StatsPlots</span><span class="x">,</span> <span class="n">Random</span>

<span class="c"># Set a random seed.</span>
<span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="x">(</span><span class="mi">3</span><span class="x">)</span>

<span class="c"># Construct 30 data points for each cluster.</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">30</span>

<span class="c"># Parameters for each cluster, we assume that each cluster is Gaussian distributed in the example.</span>
<span class="n">μs</span> <span class="o">=</span> <span class="x">[</span><span class="o">-</span><span class="mf">3.5</span><span class="x">,</span> <span class="mf">0.0</span><span class="x">]</span>

<span class="c"># Construct the data points.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">mapreduce</span><span class="x">(</span><span class="n">c</span> <span class="o">-&gt;</span> <span class="n">rand</span><span class="x">(</span><span class="n">MvNormal</span><span class="x">([</span><span class="n">μs</span><span class="x">[</span><span class="n">c</span><span class="x">],</span> <span class="n">μs</span><span class="x">[</span><span class="n">c</span><span class="x">]],</span> <span class="mf">1.</span><span class="x">),</span> <span class="n">N</span><span class="x">),</span> <span class="n">hcat</span><span class="x">,</span> <span class="mi">1</span><span class="o">:</span><span class="mi">2</span><span class="x">)</span>

<span class="c"># Visualization.</span>
<span class="n">scatter</span><span class="x">(</span><span class="n">x</span><span class="x">[</span><span class="mi">1</span><span class="x">,</span><span class="o">:</span><span class="x">],</span> <span class="n">x</span><span class="x">[</span><span class="mi">2</span><span class="x">,</span><span class="o">:</span><span class="x">],</span> <span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="x">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s">"Synthetic Dataset"</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="../1_GaussianMixtureModel_files/1_GaussianMixtureModel_2_0.svg" alt="svg" /></p>

<h2 id="gaussian-mixture-model-in-turing">Gaussian Mixture Model in Turing</h2>

<p>To cluster the data points shown above, we use a model that consists of two mixture components (clusters) and assigns each datum to one of the components. The assignment thereof determines the distribution that the data point is generated from.</p>

<p>In particular, in a Bayesian Gaussian mixture model with \(1 \leq k \leq K\) components for 1-D data each data point \(x_i\) with \(1 \leq i \leq N\) is generated according to the following generative process.
First we draw the parameters for each cluster, i.e. in our example we draw location of the distributions from a Normal:
$$
\mu_k \sim Normal() \, , \;  \forall k <br />
$$
and then draw mixing weight for the \(K\) clusters from a Dirichlet distribution, i.e.
$$
    w \sim Dirichlet(K, \alpha) \, . <br />
$$
After having constructed all the necessary model parameters, we can generate an observation by first selecting one of the clusters and then drawing the datum accordingly, i.e.
$$
    z_i \sim Categorical(w) \, , \;  \forall i <br />
    x_i \sim Normal(\mu_{z_i}, 1.) \, , \;  \forall i
$$</p>

<p>For more details on Gaussian mixture models, we refer to Christopher M. Bishop, <em>Pattern Recognition and Machine Learning</em>, Section 9.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Turing</span><span class="x">,</span> <span class="n">MCMCChains</span>

<span class="c"># Turn off the progress monitor.</span>
<span class="n">Turing</span><span class="o">.</span><span class="n">turnprogress</span><span class="x">(</span><span class="nb">false</span><span class="x">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌ Info: [Turing]: progress logging is disabled globally
└ @ Turing /home/cameron/.julia/packages/Turing/cReBm/src/Turing.jl:22





false
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@model</span> <span class="n">GaussianMixtureModel</span><span class="x">(</span><span class="n">x</span><span class="x">)</span> <span class="o">=</span> <span class="k">begin</span>
    
    <span class="n">D</span><span class="x">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">size</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>

    <span class="c"># Draw the parameters for cluster 1.</span>
    <span class="n">μ1</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">()</span>
    
    <span class="c"># Draw the parameters for cluster 2.</span>
    <span class="n">μ2</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">()</span>
    
    <span class="n">μ</span> <span class="o">=</span> <span class="x">[</span><span class="n">μ1</span><span class="x">,</span> <span class="n">μ2</span><span class="x">]</span>
    
    <span class="c"># Uncomment the following lines to draw the weights for the K clusters </span>
    <span class="c"># from a Dirichlet distribution.</span>
    
    <span class="c"># α = 1.0</span>
    <span class="c"># w ~ Dirichlet(2, α)</span>
    
    <span class="c"># Comment out this line if you instead want to draw the weights.</span>
    <span class="n">w</span> <span class="o">=</span> <span class="x">[</span><span class="mf">0.5</span><span class="x">,</span> <span class="mf">0.5</span><span class="x">]</span>
    
    <span class="c"># Draw assignments for each datum and generate it from a multivariate normal.</span>
    <span class="n">k</span> <span class="o">=</span> <span class="kt">Vector</span><span class="x">{</span><span class="kt">Int</span><span class="x">}(</span><span class="nb">undef</span><span class="x">,</span> <span class="n">N</span><span class="x">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">N</span>
        <span class="n">k</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Categorical</span><span class="x">(</span><span class="n">w</span><span class="x">)</span>
        <span class="n">x</span><span class="x">[</span><span class="o">:</span><span class="x">,</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span><span class="err"> </span><span class="n">MvNormal</span><span class="x">([</span><span class="n">μ</span><span class="x">[</span><span class="n">k</span><span class="x">[</span><span class="n">i</span><span class="x">]],</span> <span class="n">μ</span><span class="x">[</span><span class="n">k</span><span class="x">[</span><span class="n">i</span><span class="x">]]],</span> <span class="mf">1.</span><span class="x">)</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">k</span>
<span class="k">end</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##GaussianMixtureModel#361 (generic function with 2 methods)
</code></pre></div></div>

<p>After having specified the model in Turing, we can construct the model function and run a MCMC simulation to obtain assignments of the data points.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gmm_model</span> <span class="o">=</span> <span class="n">GaussianMixtureModel</span><span class="x">(</span><span class="n">x</span><span class="x">);</span>
</code></pre></div></div>

<p>To draw observations from the posterior distribution, we use a <a href="https://www.stats.ox.ac.uk/~doucet/andrieu_doucet_holenstein_PMCMC.pdf">particle Gibbs</a> sampler to draw the discrete assignment parameters as well as a Hamiltonion Monte Carlo sampler for continous parameters.</p>

<p>Note that we use a <code class="language-plaintext highlighter-rouge">Gibbs</code> sampler to combine both samplers for Bayesian inference in our model. We are also calling <code class="language-plaintext highlighter-rouge">mapreduce</code> to generate multiple chains, particularly so we test for convergence. The <code class="language-plaintext highlighter-rouge">chainscat</code> function simply adds multiple chains together.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gmm_sampler</span> <span class="o">=</span> <span class="n">Gibbs</span><span class="x">(</span><span class="n">PG</span><span class="x">(</span><span class="mi">100</span><span class="x">,</span> <span class="o">:</span><span class="n">k</span><span class="x">),</span> <span class="n">HMC</span><span class="x">(</span><span class="mf">0.05</span><span class="x">,</span> <span class="mi">10</span><span class="x">,</span> <span class="o">:</span><span class="n">μ1</span><span class="x">,</span> <span class="o">:</span><span class="n">μ2</span><span class="x">))</span>
<span class="n">tchain</span> <span class="o">=</span> <span class="n">mapreduce</span><span class="x">(</span><span class="n">c</span> <span class="o">-&gt;</span> <span class="n">sample</span><span class="x">(</span><span class="n">gmm_model</span><span class="x">,</span> <span class="n">gmm_sampler</span><span class="x">,</span> <span class="mi">100</span><span class="x">),</span> <span class="n">chainscat</span><span class="x">,</span> <span class="mi">1</span><span class="o">:</span><span class="mi">3</span><span class="x">);</span>
</code></pre></div></div>

<h2 id="visualize-the-density-region-of-the-mixture-model">Visualize the Density Region of the Mixture Model</h2>

<p>After sucessfully doing posterior inference, we can first visualize the trace and density of the parameters of interest.</p>

<p>In particular, in this example we consider the sample values of the location parameter for the two clusters.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ids</span> <span class="o">=</span> <span class="n">findall</span><span class="x">(</span><span class="n">map</span><span class="x">(</span><span class="n">name</span> <span class="o">-&gt;</span> <span class="n">occursin</span><span class="x">(</span><span class="s">"μ"</span><span class="x">,</span> <span class="n">name</span><span class="x">),</span> <span class="n">names</span><span class="x">(</span><span class="n">tchain</span><span class="x">)));</span>
<span class="n">p</span><span class="o">=</span><span class="n">plot</span><span class="x">(</span><span class="n">tchain</span><span class="x">[</span><span class="o">:</span><span class="x">,</span> <span class="n">ids</span><span class="x">,</span> <span class="o">:</span><span class="x">],</span> <span class="n">legend</span><span class="o">=</span><span class="nb">true</span><span class="x">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="x">[</span><span class="s">"Mu 1"</span> <span class="s">"Mu 2"</span><span class="x">],</span> <span class="n">colordim</span><span class="o">=:</span><span class="n">parameter</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="../1_GaussianMixtureModel_files/1_GaussianMixtureModel_13_0.svg" alt="svg" /></p>

<p>You’ll note here that it appears the location means are switching between chains. We will address this in future tutorials. For those who are keenly interested, see <a href="https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html">this</a> article on potential solutions.</p>

<p>For the moment, we will just use the first chain to ensure the validity of our inference.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tchain</span> <span class="o">=</span> <span class="n">tchain</span><span class="x">[</span><span class="o">:</span><span class="x">,</span> <span class="o">:</span><span class="x">,</span> <span class="mi">1</span><span class="x">];</span>
</code></pre></div></div>

<p>As the samples for the location parameter for both clusters are unimodal, we can safely visualize the density region of our model using the average location.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Helper function used for visualizing the density region.</span>
<span class="k">function</span><span class="nf"> predict</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">y</span><span class="x">,</span> <span class="n">w</span><span class="x">,</span> <span class="n">μ</span><span class="x">)</span>
    <span class="c"># Use log-sum-exp trick for numeric stability.</span>
    <span class="k">return</span> <span class="n">Turing</span><span class="o">.</span><span class="n">logaddexp</span><span class="x">(</span>
        <span class="n">log</span><span class="x">(</span><span class="n">w</span><span class="x">[</span><span class="mi">1</span><span class="x">])</span> <span class="o">+</span> <span class="n">logpdf</span><span class="x">(</span><span class="n">MvNormal</span><span class="x">([</span><span class="n">μ</span><span class="x">[</span><span class="mi">1</span><span class="x">],</span> <span class="n">μ</span><span class="x">[</span><span class="mi">1</span><span class="x">]],</span> <span class="mf">1.</span><span class="x">),</span> <span class="x">[</span><span class="n">x</span><span class="x">,</span> <span class="n">y</span><span class="x">]),</span> 
        <span class="n">log</span><span class="x">(</span><span class="n">w</span><span class="x">[</span><span class="mi">2</span><span class="x">])</span> <span class="o">+</span> <span class="n">logpdf</span><span class="x">(</span><span class="n">MvNormal</span><span class="x">([</span><span class="n">μ</span><span class="x">[</span><span class="mi">2</span><span class="x">],</span> <span class="n">μ</span><span class="x">[</span><span class="mi">2</span><span class="x">]],</span> <span class="mf">1.</span><span class="x">),</span> <span class="x">[</span><span class="n">x</span><span class="x">,</span> <span class="n">y</span><span class="x">])</span>
    <span class="x">)</span>
<span class="k">end</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>predict (generic function with 1 method)
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">contour</span><span class="x">(</span><span class="n">range</span><span class="x">(</span><span class="o">-</span><span class="mi">5</span><span class="x">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="mi">3</span><span class="x">),</span> <span class="n">range</span><span class="x">(</span><span class="o">-</span><span class="mi">6</span><span class="x">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="mi">2</span><span class="x">),</span> 
    <span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">y</span><span class="x">)</span> <span class="o">-&gt;</span> <span class="n">predict</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">y</span><span class="x">,</span> <span class="x">[</span><span class="mf">0.5</span><span class="x">,</span> <span class="mf">0.5</span><span class="x">],</span> <span class="x">[</span><span class="n">mean</span><span class="x">(</span><span class="n">tchain</span><span class="x">[</span><span class="o">:</span><span class="n">μ1</span><span class="x">]</span><span class="o">.</span><span class="n">value</span><span class="x">),</span> <span class="n">mean</span><span class="x">(</span><span class="n">tchain</span><span class="x">[</span><span class="o">:</span><span class="n">μ2</span><span class="x">]</span><span class="o">.</span><span class="n">value</span><span class="x">)])</span>
<span class="x">)</span>
<span class="n">scatter!</span><span class="x">(</span><span class="n">x</span><span class="x">[</span><span class="mi">1</span><span class="x">,</span><span class="o">:</span><span class="x">],</span> <span class="n">x</span><span class="x">[</span><span class="mi">2</span><span class="x">,</span><span class="o">:</span><span class="x">],</span> <span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="x">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s">"Synthetic Dataset"</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="../1_GaussianMixtureModel_files/1_GaussianMixtureModel_18_0.svg" alt="svg" /></p>

<h2 id="infered-assignments">Infered Assignments</h2>

<p>Finally, we can inspect the assignments of the data points infered using Turing. As we can see, the dataset is partitioned into two distinct groups.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">assignments</span> <span class="o">=</span> <span class="n">collect</span><span class="x">(</span><span class="n">skipmissing</span><span class="x">(</span><span class="n">mean</span><span class="x">(</span><span class="n">tchain</span><span class="x">[</span><span class="o">:</span><span class="n">k</span><span class="x">]</span><span class="o">.</span><span class="n">value</span><span class="x">,</span> <span class="n">dims</span><span class="o">=</span><span class="mi">1</span><span class="x">)</span><span class="o">.</span><span class="n">data</span><span class="x">))</span>
<span class="n">scatter</span><span class="x">(</span><span class="n">x</span><span class="x">[</span><span class="mi">1</span><span class="x">,</span><span class="o">:</span><span class="x">],</span> <span class="n">x</span><span class="x">[</span><span class="mi">2</span><span class="x">,</span><span class="o">:</span><span class="x">],</span> 
    <span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="x">,</span> 
    <span class="n">title</span> <span class="o">=</span> <span class="s">"Assignments on Synthetic Dataset"</span><span class="x">,</span> 
    <span class="n">zcolor</span> <span class="o">=</span> <span class="n">assignments</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="../1_GaussianMixtureModel_files/1_GaussianMixtureModel_21_0.svg" alt="svg" /></p>

:ET