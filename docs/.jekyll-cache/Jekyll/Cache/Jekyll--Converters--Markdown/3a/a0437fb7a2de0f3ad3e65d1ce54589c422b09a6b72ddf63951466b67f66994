I"`<h1 id="probabilistic-modelling-using-the-infinite-mixture-model">Probabilistic Modelling using the Infinite Mixture Model</h1>

<p>In many applications it is desirable to allow the model to adjust its complexity to the amount the data. Consider for example the task of assigning objects into clusters or groups. This task often involves the specification of the number of groups. However, often times it is not known beforehand how many groups exist. Moreover, in some applictions, e.g. modelling topics in text documents or grouping species, the number of examples per group is heavy tailed. This makes it impossible to predefine the number of groups and requiring the model to form new groups when data points from previously unseen groups are observed.</p>

<p>A natural approach for such applications is the use of non-parametric models. This tutorial will introduce how to use the Dirichlet process in a mixture of infinitely many Gaussians using Turing. For further information on Bayesian nonparametrics and the Dirichlet process we refer to the <a href="http://mlg.eng.cam.ac.uk/pub/pdf/Gha12.pdf">introduction by Zoubin Ghahramani</a> and the book “Fundamentals of Nonparametric Bayesian Inference” by Subhashis Ghosal and Aad van der Vaart.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Turing</span>
</code></pre></div></div>

<h2 id="mixture-model">Mixture Model</h2>

<p>Before introducing infinite mixture models in Turing, we will briefly review the construction of finite mixture models. Subsequently, we will define how to use the <a href="https://en.wikipedia.org/wiki/Chinese_restaurant_process">Chinese restaurant process</a> construction of a Dirichlet process for non-parametric clustering.</p>

<h4 id="two-component-model">Two-Component Model</h4>

<p>First, consider the simple case of a mixture model with two Gaussian components with fixed covariance. 
The generative process of such a model can be written as:</p>

\[\begin{align}
\pi_1 &amp;\sim Beta(a, b) \\
\pi_2 &amp;= 1-\pi_1 \\
\mu_1 &amp;\sim Normal(\mu_0, \Sigma_0) \\
\mu_2 &amp;\sim Normal(\mu_0, \Sigma_0) \\
z_i &amp;\sim Categorical(\pi_1, \pi_2) \\
x_i &amp;\sim Normal(\mu_{z_i}, \Sigma)
\end{align}\]

<p>where \(\pi_1, \pi_2\) are the mixing weights of the mixture model, i.e. \(\pi_1 + \pi_2 = 1\), and \(z_i\) is a latent assignment of the observation \(x_i\) to a component (Gaussian).</p>

<p>We can implement this model in Turing for 1D data as follows:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@model</span> <span class="n">two_model</span><span class="x">(</span><span class="n">x</span><span class="x">)</span> <span class="o">=</span> <span class="k">begin</span>
    
    <span class="c"># Hyper-parameters</span>
    <span class="n">μ0</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">σ0</span> <span class="o">=</span> <span class="mf">1.0</span>
    
    <span class="c"># Draw weights.</span>
    <span class="n">π1</span> <span class="o">~</span><span class="err"> </span><span class="n">Beta</span><span class="x">(</span><span class="mi">1</span><span class="x">,</span><span class="mi">1</span><span class="x">)</span>
    <span class="n">π2</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">π1</span>
    
    <span class="c"># Draw locations of the components.</span>
    <span class="n">μ1</span> <span class="o">~</span><span class="err"> </span><span class="n">Normal</span><span class="x">(</span><span class="n">μ0</span><span class="x">,</span> <span class="n">σ0</span><span class="x">)</span>
    <span class="n">μ2</span> <span class="o">~</span><span class="err"> </span><span class="n">Normal</span><span class="x">(</span><span class="n">μ0</span><span class="x">,</span> <span class="n">σ0</span><span class="x">)</span>
    
    <span class="c"># Draw latent assignment.</span>
    <span class="n">z</span> <span class="o">~</span><span class="err"> </span><span class="n">Categorical</span><span class="x">([</span><span class="n">π1</span><span class="x">,</span> <span class="n">π2</span><span class="x">])</span>
    
    <span class="c"># Draw observation from selected component.</span>
    <span class="k">if</span> <span class="n">z</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="n">x</span> <span class="o">~</span><span class="err"> </span><span class="n">Normal</span><span class="x">(</span><span class="n">μ1</span><span class="x">,</span> <span class="mf">1.0</span><span class="x">)</span>
    <span class="k">else</span>
        <span class="n">x</span> <span class="o">~</span><span class="err"> </span><span class="n">Normal</span><span class="x">(</span><span class="n">μ2</span><span class="x">,</span> <span class="mf">1.0</span><span class="x">)</span>
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DynamicPPL.ModelGen{var"###generator#282",(:x,),(),Tuple{}}(##generator#282, NamedTuple())
</code></pre></div></div>

<h4 id="finite-mixture-model">Finite Mixture Model</h4>

<p>If we have more than two components, this model can elegantly be extend using a Dirichlet distribution as prior for the mixing weights \(\pi_1, \dots, \pi_K\). Note that the Dirichlet distribution is the multivariate generalization of the beta distribution. The resulting model can be written as:</p>

\[\begin{align}
(\pi_1, \dots, \pi_K) &amp;\sim Dirichlet(K, \alpha) \\
\mu_k &amp;\sim Normal(\mu_0, \Sigma_0), \;\; \forall k \\
z &amp;\sim Categorical(\pi_1, \dots, \pi_K) \\
x &amp;\sim Normal(\mu_z, \Sigma) 
\end{align}\]

<p>which resembles the model in the <a href="https://turing.ml/dev/tutorials/1-gaussianmixturemodel/">Gaussian mixture model tutorial</a> with a slightly different notation.</p>

<h2 id="infinite-mixture-model">Infinite Mixture Model</h2>

<p>The question now arises, is there a generalization of a Dirichlet distribution for which the dimensionality \(K\) is infinite, i.e. \(K = \infty\)?</p>

<p>But first, to implement an infinite Gaussian mixture model in Turing, we first need to load the <code class="language-plaintext highlighter-rouge">Turing.RandomMeasures</code> module. <code class="language-plaintext highlighter-rouge">RandomMeasures</code> contains a variety of tools useful in nonparametrics.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Turing</span><span class="o">.</span><span class="n">RandomMeasures</span>
</code></pre></div></div>

<p>We now will utilize the fact that one can integrate out the mixing weights in a Gaussian mixture model allowing us to arrive at the Chinese restaurant process construction. See Carl E. Rasmussen: <a href="https://www.seas.harvard.edu/courses/cs281/papers/rasmussen-1999a.pdf">The Infinite Gaussian Mixture Model</a>, NIPS (2000) for details.</p>

<p>In fact, if the mixing weights are integrated out, the conditional prior for the latent variable \(z\) is given by:</p>

<p>$$ 
p(z_i = k \mid z_{\not i}, \alpha) = \frac{n_k + \alpha/K}{N - 1 + \alpha}
$$</p>

<p>where \(z_{\not i}\) are the latent assignments of all observations except observation \(i\). Note that we use \(n_k\) to denote the number of observations at component \(k\) excluding observation \(i\). The parameter \(\alpha\) is the concentration parameter of the Dirichlet distribution used as prior over the mixing weights.</p>

<h4 id="chinese-restaurant-process">Chinese Restaurant Process</h4>

<p>To obtain the Chinese restaurant process construction, we can now derive the conditional prior if \(K \rightarrow \infty\).</p>

<p>For \(n_k &gt; 0\) we obtain:</p>

<p>$$
p(z_i = k \mid z_{\not i}, \alpha) = \frac{n_k}{N - 1 + \alpha}
$$</p>

<p>and for all infinitely many clusters that are empty (combined) we get:</p>

<p>$$
p(z_i = k \mid z_{\not i}, \alpha) = \frac{\alpha}{N - 1 + \alpha}
$$</p>

<p>Those equations show that the conditional prior for component assignments is proportional to the number of such observations, meaning that the Chinese restaurant process has a rich get richer property.</p>

<p>To get a better understanding of this property, we can plot the cluster choosen by for each new observation drawn from the conditional prior.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Concentration parameter.</span>
<span class="n">α</span> <span class="o">=</span> <span class="mf">10.0</span>

<span class="c"># Random measure, e.g. Dirichlet process.</span>
<span class="n">rpm</span> <span class="o">=</span> <span class="n">DirichletProcess</span><span class="x">(</span><span class="n">α</span><span class="x">)</span>

<span class="c"># Cluster assignments for each observation.</span>
<span class="n">z</span> <span class="o">=</span> <span class="kt">Vector</span><span class="x">{</span><span class="kt">Int</span><span class="x">}()</span>

<span class="c"># Maximum number of observations we observe.</span>
<span class="n">Nmax</span> <span class="o">=</span> <span class="mi">500</span>

<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">Nmax</span>
    <span class="c"># Number of observations per cluster.</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">isempty</span><span class="x">(</span><span class="n">z</span><span class="x">)</span> <span class="o">?</span> <span class="mi">0</span> <span class="o">:</span> <span class="n">maximum</span><span class="x">(</span><span class="n">z</span><span class="x">)</span>
    <span class="n">nk</span> <span class="o">=</span> <span class="kt">Vector</span><span class="x">{</span><span class="kt">Int</span><span class="x">}(</span><span class="n">map</span><span class="x">(</span><span class="n">k</span> <span class="o">-&gt;</span> <span class="n">sum</span><span class="x">(</span><span class="n">z</span> <span class="o">.==</span> <span class="n">k</span><span class="x">),</span> <span class="mi">1</span><span class="o">:</span><span class="n">K</span><span class="x">))</span>
    
    <span class="c"># Draw new assignment.</span>
    <span class="n">push!</span><span class="x">(</span><span class="n">z</span><span class="x">,</span> <span class="n">rand</span><span class="x">(</span><span class="n">ChineseRestaurantProcess</span><span class="x">(</span><span class="n">rpm</span><span class="x">,</span> <span class="n">nk</span><span class="x">)))</span>
<span class="k">end</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Plots</span>

<span class="c"># Plot the cluster assignments over time </span>
<span class="nd">@gif</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">Nmax</span>
    <span class="n">scatter</span><span class="x">(</span><span class="n">collect</span><span class="x">(</span><span class="mi">1</span><span class="o">:</span><span class="n">i</span><span class="x">),</span> <span class="n">z</span><span class="x">[</span><span class="mi">1</span><span class="o">:</span><span class="n">i</span><span class="x">],</span> <span class="n">markersize</span> <span class="o">=</span> <span class="mi">2</span><span class="x">,</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="s">"observation (i)"</span><span class="x">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s">"cluster (k)"</span><span class="x">,</span> <span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="x">)</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌ Info: Saved animation to 
│   fn = /home/cameron/code/TuringTutorials/tmp.gif
└ @ Plots /home/cameron/.julia/packages/Plots/Xnzc7/src/animation.jl:104
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/422990/55284032-90cfa980-5323-11e9-8a99-f9315db170cb.gif" alt="tmp" /></p>

<p>Further, we can see that the number of clusters is logarithmic in the number of observations and data points. This is a side-effect of the “rich-get-richer” phenomenon, i.e. we expect large clusters and thus the number of clusters has to be smaller than the number of observations.</p>

<p>$$
E[K \mid N] \approx \alpha \cdot log \big(1 + \frac{N}{\alpha}\big)
$$</p>

<p>We can see from the equation that the concentration parameter \(\alpha\) allows us to control the number of clusters formed <em>a priori</em>.</p>

<p>In Turing we can implement an infinite Gaussian mixture model using the Chinese restaurant process construction of a Dirichlet process as follows:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@model</span> <span class="n">infiniteGMM</span><span class="x">(</span><span class="n">x</span><span class="x">)</span> <span class="o">=</span> <span class="k">begin</span>
    
    <span class="c"># Hyper-parameters, i.e. concentration parameter and parameters of H.</span>
    <span class="n">α</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">μ0</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">σ0</span> <span class="o">=</span> <span class="mf">1.0</span>
    
    <span class="c"># Define random measure, e.g. Dirichlet process.</span>
    <span class="n">rpm</span> <span class="o">=</span> <span class="n">DirichletProcess</span><span class="x">(</span><span class="n">α</span><span class="x">)</span>
    
    <span class="c"># Define the base distribution, i.e. expected value of the Dirichlet process.</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">Normal</span><span class="x">(</span><span class="n">μ0</span><span class="x">,</span> <span class="n">σ0</span><span class="x">)</span>
    
    <span class="c"># Latent assignment.</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tzeros</span><span class="x">(</span><span class="kt">Int</span><span class="x">,</span> <span class="n">length</span><span class="x">(</span><span class="n">x</span><span class="x">))</span>
        
    <span class="c"># Locations of the infinitely many clusters.</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">tzeros</span><span class="x">(</span><span class="kt">Float64</span><span class="x">,</span> <span class="mi">0</span><span class="x">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
        
        <span class="c"># Number of clusters.</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">maximum</span><span class="x">(</span><span class="n">z</span><span class="x">)</span>
        <span class="n">nk</span> <span class="o">=</span> <span class="kt">Vector</span><span class="x">{</span><span class="kt">Int</span><span class="x">}(</span><span class="n">map</span><span class="x">(</span><span class="n">k</span> <span class="o">-&gt;</span> <span class="n">sum</span><span class="x">(</span><span class="n">z</span> <span class="o">.==</span> <span class="n">k</span><span class="x">),</span> <span class="mi">1</span><span class="o">:</span><span class="n">K</span><span class="x">))</span>

        <span class="c"># Draw the latent assignment.</span>
        <span class="n">z</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span><span class="err"> </span><span class="n">ChineseRestaurantProcess</span><span class="x">(</span><span class="n">rpm</span><span class="x">,</span> <span class="n">nk</span><span class="x">)</span>
        
        <span class="c"># Create a new cluster?</span>
        <span class="k">if</span> <span class="n">z</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">&gt;</span> <span class="n">K</span>
            <span class="n">push!</span><span class="x">(</span><span class="n">μ</span><span class="x">,</span> <span class="mf">0.0</span><span class="x">)</span>

            <span class="c"># Draw location of new cluster.</span>
            <span class="n">μ</span><span class="x">[</span><span class="n">z</span><span class="x">[</span><span class="n">i</span><span class="x">]]</span> <span class="o">~</span><span class="err"> </span><span class="n">H</span>
        <span class="k">end</span>
                
        <span class="c"># Draw observation.</span>
        <span class="n">x</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">μ</span><span class="x">[</span><span class="n">z</span><span class="x">[</span><span class="n">i</span><span class="x">]],</span> <span class="mf">1.0</span><span class="x">)</span>
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DynamicPPL.ModelGen{var"###generator#800",(:x,),(),Tuple{}}(##generator#800, NamedTuple())
</code></pre></div></div>

<p>We can now use Turing to infer the assignments of some data points. First, we will create some random data that comes from three clusters, with means of 0, -5, and 10.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Plots</span><span class="x">,</span> <span class="n">Random</span>

<span class="c"># Generate some test data.</span>
<span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="x">(</span><span class="mi">1</span><span class="x">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="n">randn</span><span class="x">(</span><span class="mi">10</span><span class="x">),</span> <span class="n">randn</span><span class="x">(</span><span class="mi">10</span><span class="x">)</span> <span class="o">.-</span> <span class="mi">5</span><span class="x">,</span> <span class="n">randn</span><span class="x">(</span><span class="mi">10</span><span class="x">)</span> <span class="o">.+</span> <span class="mi">10</span><span class="x">)</span>
<span class="n">data</span> <span class="o">.-=</span> <span class="n">mean</span><span class="x">(</span><span class="n">data</span><span class="x">)</span>
<span class="n">data</span> <span class="o">/=</span> <span class="n">std</span><span class="x">(</span><span class="n">data</span><span class="x">);</span>
</code></pre></div></div>

<p>Next, we’ll sample from our posterior using SMC.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># MCMC sampling</span>
<span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="x">(</span><span class="mi">2</span><span class="x">)</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">model_fun</span> <span class="o">=</span> <span class="n">infiniteGMM</span><span class="x">(</span><span class="n">data</span><span class="x">);</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">model_fun</span><span class="x">,</span> <span class="n">SMC</span><span class="x">(),</span> <span class="n">iterations</span><span class="x">);</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00[39m
</code></pre></div></div>

<p>Finally, we can plot the number of clusters in each sample.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Extract the number of clusters for each sample of the Markov chain.</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">map</span><span class="x">(</span><span class="n">t</span> <span class="o">-&gt;</span> <span class="n">length</span><span class="x">(</span><span class="n">unique</span><span class="x">(</span><span class="n">chain</span><span class="x">[</span><span class="o">:</span><span class="n">z</span><span class="x">]</span><span class="o">.</span><span class="n">value</span><span class="x">[</span><span class="n">t</span><span class="x">,</span><span class="o">:</span><span class="x">,</span><span class="o">:</span><span class="x">])),</span> <span class="mi">1</span><span class="o">:</span><span class="n">iterations</span><span class="x">);</span>

<span class="c"># Visualize the number of clusters.</span>
<span class="n">plot</span><span class="x">(</span><span class="n">k</span><span class="x">,</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="s">"Iteration"</span><span class="x">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s">"Number of clusters"</span><span class="x">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"Chain 1"</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="../6_InfiniteMixtureModel_files/6_InfiniteMixtureModel_31_0.svg" alt="svg" /></p>

<p>If we visualize the histogram of the number of clusters sampled from our posterior, we observe that the model seems to prefer 3 clusters, which is the true number of clusters. Note that the number of clusters in a Dirichlet process mixture model is not limited a priori and will grow to infinity with probability one. However, if conditioned on data the posterior will concentrate on a finite number of clusters enforcing the resulting model to have a finite amount of clusters. It is, however, not given that the posterior of a Dirichlet process Gaussian mixture model converges to the true number of clusters, given that data comes from a finite mixture model. See Jeffrey Miller and Matthew Harrison: <a href="https://arxiv.org/pdf/1301.2708.pdf">A simple example of Dirichlet process mixture inconsitency for the number of components</a> for details.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">histogram</span><span class="x">(</span><span class="n">k</span><span class="x">,</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="s">"Number of clusters"</span><span class="x">,</span> <span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="x">)</span>
</code></pre></div></div>

<p><img src="../6_InfiniteMixtureModel_files/6_InfiniteMixtureModel_33_0.svg" alt="svg" /></p>

<p>One issue with the Chinese restaurant process construction is that the number of latent parameters we need to sample scales with the number of observations. It may be desirable to use alternative constructions in certain cases. Alternative methods of constructing a Dirichlet process can be employed via the following representations:</p>

<p>Size-Biased Sampling Process</p>

<p>$$
j_k \sim Beta(1, \alpha) * surplus
$$</p>

<p>Stick-Breaking Process
$$
v_k \sim Beta(1, \alpha)
$$</p>

<p>Chinese Restaurant Process
$$
p(z_n = k | z_{1:n-1}) \propto \begin{cases} 
        \frac{m_k}{n-1+\alpha}, \text{ if } m_k &gt; 0\<br />
        \frac{\alpha}{n-1+\alpha}
    \end{cases}
$$</p>

<p>For more details see <a href="https://www.stats.ox.ac.uk/~teh/research/npbayes/Teh2010a.pdf">this article</a>.</p>
:ET